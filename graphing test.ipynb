{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entree Task: Implementing Your Own Neural Networks from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt # for graphics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Linear Layer \n",
    "Implement the forward and backward functions for a linear layer. Please read the requirement details for Task 1 in the code comment and in the pdf document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    def __init__(self, _m, _n):\n",
    "        '''\n",
    "        :param _m: _m is the input X hidden size\n",
    "        :param _n: _n is the output Y hidden size\n",
    "        '''\n",
    "        # \"Kaiming initialization\" is important for neural network to converge. The NN will not converge without it!\n",
    "        self.W = (np.random.uniform(low=-10000.0, high=10000.0, size = (_m, _n)))/10000.0*np.sqrt(6.0/ _m)\n",
    "        self.stored_X = None\n",
    "        self.W_grad = None #record the gradient of the weight\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        :param X: shape(X)[0] is batch size and shape(X)[1] is the #features\n",
    "         (1) Store the input X in stored_data for Backward.\n",
    "         (2) :return: X * weights\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        self.stored_X = X\n",
    "        return X @ self.W\n",
    "        \n",
    "        ##########  Code end   ##########\n",
    "    \n",
    "    def backward(self, Y_grad):\n",
    "        '''\n",
    "        /* shape(output_grad)[0] is batch size and shape(output_grad)[1] is the # output features (shape(weight)[1])\n",
    "         * 1) Calculate the gradient of the output (the result of the Forward method) w.r.t. the **W** and store the product of the gradient and Y_grad in W_grad\n",
    "         * 2) Calculate the gradient of the output (the result of the Forward method) w.r.t. the **X** and return the product of the gradient and Y_grad\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        self.W_grad = self.stored_X.T @ Y_grad\n",
    "        return Y_grad @ self.W.T\n",
    "        ##########  Code end   ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 1: Linear Layer\n",
    "Check your linear forward and backward function implementations with numerical derivatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your gradient:  [[ 0.12886674 -0.27329106 -2.62930584]]\n",
      "Numerical gradient: [[ 0.12886674 -0.27329106 -2.62930584]]\n",
      "Error:  8.876532842094775e-11\n",
      "Correct backward. Congratulations!\n"
     ]
    }
   ],
   "source": [
    "#gradient check\n",
    "import copy\n",
    "#Random test\n",
    "n = 3\n",
    "m = 6\n",
    "Y_grad = np.random.rand(1, m)\n",
    "test_vector = np.random.rand(1, n)\n",
    "DELTA = 1e-6\n",
    "test_layer = LinearLayer(n, m)\n",
    "\n",
    "test_layer_1 = copy.deepcopy(test_layer)\n",
    "test_layer_2 = copy.deepcopy(test_layer)\n",
    "\n",
    "test_layer.forward(test_vector)\n",
    "Your_backward = test_layer.backward(Y_grad)\n",
    "\n",
    "cal_gradient = np.zeros((np.shape(test_vector)[0], np.shape(test_vector)[1]))\n",
    "for t_p in range(np.shape(test_vector)[0]):\n",
    "    for i in range(np.shape(test_vector)[1]):\n",
    "        test_vector_1 = copy.deepcopy(test_vector)\n",
    "        test_vector_2 = copy.deepcopy(test_vector)\n",
    "        test_vector_1[t_p][i] = test_vector_1[t_p][i] + DELTA\n",
    "        test_vector_2[t_p][i] = test_vector_2[t_p][i] - DELTA\n",
    "\n",
    "        cal_gradient[t_p][i] = np.sum(\n",
    "            (np.dot(Y_grad, np.transpose(test_layer_1.forward(test_vector_1) - test_layer_2.forward(test_vector_2))/(2*DELTA))))\n",
    "\n",
    "\n",
    "print('Your gradient: ',Your_backward)\n",
    "print('Numerical gradient:',cal_gradient)\n",
    "print('Error: ',abs(np.sum(Your_backward - cal_gradient)))\n",
    "if abs(np.sum(Your_backward - cal_gradient)) < 1e-4:\n",
    "    print('Correct backward. Congratulations!')\n",
    "else:\n",
    "    print('Wrong backawrd. Please check your implementation again.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Non-Linear Activation\n",
    "Implement the forward and backward functions for a nonlinear layer. Please read the requirement details for Task 2 in the code comment and in the pdf document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    #sigmoid layer\n",
    "    def __init__(self):\n",
    "        self.stored_X = None # Here we should store the input matrix X for Backward\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        /*\n",
    "         *  The input X matrix has the dimension [#samples, #features].\n",
    "         *  The output Y matrix has the same dimension as the input X.\n",
    "         *  You need to perform ReLU on each element of the input matrix to calculate the output matrix.\n",
    "         *  TODO: 1) Create an output matrix by going through each element in input and calculate relu=max(0,x) and\n",
    "         *  TODO: 2) Store the input X in self.stored_X for Backward.\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        self.stored_X = X\n",
    "        Y = np.zeros(X.shape)\n",
    "        for i in range(X.shape[0]):\n",
    "            for j in range(X.shape[1]):\n",
    "                Y[i][j] = max(0, X[i][j])\n",
    "        \n",
    "        return Y\n",
    "                \n",
    "        ##########  Code end   ##########\n",
    "\n",
    "    def backward(self, Y_grad):\n",
    "        '''\n",
    "         /*  grad_relu(x)=1 if relu(x)=x\n",
    "         *  grad_relu(x)=0 if relu(x)=0\n",
    "         *\n",
    "         *  The input matrix has the name \"output_grad.\" The name is confusing (it is actually the input of the function). But the name follows the convension in PyTorch.\n",
    "         *  The output matrix has the same dimension as input.\n",
    "         *  The output matrix is calculated as grad_relu(stored_X)*Y_grad.\n",
    "         *  TODO: returns the output matrix calculated above\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        grad = np.zeros(Y_grad.shape)\n",
    "        for i in range(self.stored_X.shape[0]):\n",
    "            for j in range(self.stored_X.shape[1]):\n",
    "                if (self.stored_X[i][j] > 0):\n",
    "                    grad[i][j] = Y_grad[i][j]\n",
    "                else:\n",
    "                    grad[i][j] = 0  \n",
    "        \n",
    "        return grad\n",
    "\n",
    "        ##########  Code end   ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 2: ReLU \n",
    "Check your ReLU forward and backward functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your gradient:  [[0.46025087 0.18790112 0.71212611]]\n",
      "Numerical gradient: [[0.46025087 0.18790112 0.71212611]]\n",
      "Error:  1.3566592294012025e-11\n",
      "Correct backward. Congratulations!\n"
     ]
    }
   ],
   "source": [
    "#gradient check\n",
    "import copy\n",
    "#Random test\n",
    "n = 3\n",
    "Y_grad = np.random.rand(1, n)\n",
    "test_vector = np.random.rand(1, n)\n",
    "DELTA = 1e-6\n",
    "test_layer = ReLU()\n",
    "\n",
    "test_layer_1 = copy.deepcopy(test_layer)\n",
    "test_layer_2 = copy.deepcopy(test_layer)\n",
    "\n",
    "test_layer.forward(test_vector)\n",
    "Your_backward = test_layer.backward(Y_grad)\n",
    "\n",
    "cal_gradient = np.zeros((np.shape(test_vector)[0], np.shape(test_vector)[1]))\n",
    "for t_p in range(np.shape(test_vector)[0]):\n",
    "    for i in range(np.shape(test_vector)[1]):\n",
    "        test_vector_1 = copy.deepcopy(test_vector)\n",
    "        test_vector_2 = copy.deepcopy(test_vector)\n",
    "        test_vector_1[t_p][i] = test_vector_1[t_p][i] + DELTA\n",
    "        test_vector_2[t_p][i] = test_vector_2[t_p][i] - DELTA\n",
    "\n",
    "        cal_gradient[t_p][i] = np.sum(\n",
    "            (np.dot(Y_grad, np.transpose(test_layer_1.forward(test_vector_1) - test_layer_2.forward(test_vector_2))/(2*DELTA))))\n",
    "\n",
    "\n",
    "print('Your gradient: ',Your_backward)\n",
    "print('Numerical gradient:',cal_gradient)\n",
    "print('Error: ',abs(np.sum(Your_backward - cal_gradient)))\n",
    "if abs(np.sum(Your_backward - cal_gradient)) < 1e-4:\n",
    "    print('Correct backward. Congratulations!')\n",
    "else:\n",
    "    print('Wrong backawrd. Please check your implementation again.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Loss Function\n",
    "Implement the MSE loss function and its backward derivative. Please read the requirement details for Task 3 in the code comment and in the pdf document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    # cross entropy loss\n",
    "    # return the mse loss mean(y_j-y_pred_i)^2\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stored_diff = None\n",
    "    def forward(self, prediction, groundtruth):\n",
    "        '''\n",
    "        /*  TODO: 1) Calculate stored_data=pred-truth\n",
    "         *  TODO: 2) Calculate the MSE loss as the squared sum of all the elements in the stored_data divided by the number of elements, i.e., MSE(pred, truth) = ||pred-truth||^2 / N, with N as the total number of elements in the matrix\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        self.stored_diff = prediction - groundtruth\n",
    "        sumSquare = np.sum(np.square(prediction - groundtruth))\n",
    "        return sumSquare/prediction.shape[0]\n",
    "        ##########  Code end   ##########\n",
    "\n",
    "    # return the gradient of the input data\n",
    "    def backward(self):\n",
    "        '''\n",
    "        /* TODO: return the gradient matrix of the MSE loss\n",
    "         * The output matrix has the same dimension as the stored_data (make sure you have stored the (pred-truth) in stored_data in your forward function!)\n",
    "         * Each element (i,j) of the output matrix is calculated as grad(i,j)=2(pred(i,j)-truth(i,j))/N\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        return (2/self.stored_diff.shape[0]) * self.stored_diff\n",
    "        ##########  Code end   ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Network Architecture\n",
    "Implement your own neural network architecture. Please read the requirement for Task 4 in the pdf document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, layers_arch):\n",
    "        '''\n",
    "        /*  TODO: 1) Initialize the array for input layers with the proper feature sizes specified in the input vector.\n",
    "         * For the linear layer, in each pair (in_size, out_size), the in_size is the feature size of the previous layer and the out_size is the feature size of the output (that goes to the next layer)\n",
    "         * In the linear layer, the weight should have the shape (in_size, out_size).\n",
    "         \n",
    "         *  For example, if layers_arch = [['Linear', (256, 128)], ['ReLU'], ['Linear', (128, 64)], ['ReLU'], ['Linear', (64, 32)]],\n",
    "       * \t\t\t\t\t\t\t then there are three linear layers whose weights are with shapes (256, 128), (128, 64), (64, 32),\n",
    "       * \t\t\t\t\t\t\t and there are two non-linear layers.\n",
    "         *  Attention: * The output feature size of the linear layer i should always equal to the input feature size of the linear layer i+1.\n",
    "       */\n",
    "        '''\n",
    "       \n",
    "        ########## Code start  ##########\n",
    "        self.layers = []\n",
    "\n",
    "        for layer in layers_arch:\n",
    "            if layer[0] == 'Linear':\n",
    "                self.layers.append(LinearLayer(layer[1][0], layer[1][1]))\n",
    "            else:\n",
    "                self.layers.append(ReLU())\n",
    "            \n",
    "        ##########  Code end   ##########\n",
    "        \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        /*\n",
    "         * TODO: propagate the input data for the first linear layer throught all the layers in the network and return the output of the last linear layer.\n",
    "         * For implementation, you need to write a for-loop to propagate the input from the first layer to the last layer (before the loss function) by going through the forward functions of all the layers.\n",
    "         * For example, for a network with k linear layers and k-1 activation layers, the data flow is:\n",
    "         * linear[0] -> activation[0] -> linear[1] ->activation[1] -> ... -> linear[k-2] -> activation[k-2] -> linear[k-1]\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        \n",
    "        curr_x = X\n",
    "        for i in range(len(self.layers)):\n",
    "            curr_x = self.layers[i].forward(curr_x)\n",
    "        \n",
    "        return curr_x\n",
    "\n",
    "        ##########  Code end   ##########\n",
    "\n",
    "    def backward(self, Y_grad):\n",
    "        '''\n",
    "        /* Propagate the gradient from the last layer to the first layer by going through the backward functions of all the layers.\n",
    "         * TODO: propagate the gradient of the output (we got from the Forward method) back throught the network and return the gradient of the first layer.\n",
    "\n",
    "         * Notice: We should use the chain rule for the backward.\n",
    "         * Notice: The order is opposite to the forward.\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        curr_y_grad = Y_grad\n",
    "        for i in range(len(self.layers)-1, -1, -1):\n",
    "            curr_y_grad = self.layers[i].backward(curr_y_grad)\n",
    "        \n",
    "        return curr_y_grad\n",
    "            \n",
    "        ##########  Code end   ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 3: Regression Network\n",
    "Check your network implementation with a simple regression task. Here we also provide you a sample implementation for the gradient descent algorithm, which you will find useful for your own Classifier implementation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor:\n",
    "    #Classifier\n",
    "    def __init__(self, layers_arch, data_function, learning_rate = 1e-3, batch_size = 32, max_epoch = 200):\n",
    "\n",
    "        input_feature_size = 2\n",
    "        output_feature_size = 2\n",
    "\n",
    "        self.train_data = []\n",
    "        self.train_label = []\n",
    "        self.test_data = []\n",
    "        self.test_label = []\n",
    "\n",
    "        self.data_function = data_function\n",
    "        \n",
    "        self.layers_arch = layers_arch\n",
    "        self.net = Network(layers_arch)\n",
    "        self.loss_function = MSELoss()\n",
    "\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def dataloader(self):\n",
    "        \n",
    "        '''\n",
    "        We randomly generate the mapping: (x)->(x^3+x^2 + 1)\n",
    "        '''\n",
    "        self.train_data = np.zeros((1000,1))\n",
    "        self.train_label = np.zeros((1000, 1))\n",
    "\n",
    "        for i in range(1000):\n",
    "            self.train_data[i][0] = np.random.uniform(low=0.0, high=10000.0)/10000.0\n",
    "            self.train_label[i][0] = self.data_function(self.train_data[i][0])\n",
    "\n",
    "        self.test_data = np.zeros((200, 1))\n",
    "        self.test_label = np.zeros((200, 1))\n",
    "\n",
    "        for i in range(200):\n",
    "            self.test_data[i][0] = np.random.uniform(low=-0.0, high=10000.0) / 10000.0\n",
    "            self.test_label[i][0] = self.data_function(self.test_data[i][0])\n",
    "\n",
    "\n",
    "\n",
    "    def Train_One_Epoch(self):\n",
    "        '''\n",
    "        Here we train the network using gradient descent\n",
    "        '''\n",
    "        loss = 0\n",
    "        n_loop = int(math.ceil(len(self.train_data)/self.batch_size))\n",
    "    \n",
    "\n",
    "        for i in range(n_loop):\n",
    "            batch_data = self.train_data[i * self.batch_size : (i+1)*self.batch_size]\n",
    "            batch_label = self.train_label[i * self.batch_size : (i+1)*self.batch_size]\n",
    "            \n",
    "            '''\n",
    "            /*  Forward the data to the network.\n",
    "             *  Forward the result to the loss function.\n",
    "             *  Backward.\n",
    "             *  Update the weights with weight gradients.\n",
    "             *  Do not forget the learning rate!\n",
    "             */\n",
    "            '''\n",
    "            \n",
    "            ########## Sample code  ##########\n",
    "            prediction = self.net.forward(batch_data)\n",
    "            loss += self.loss_function.forward(prediction, batch_label)\n",
    "\n",
    "            pred_grad = self.loss_function.backward()\n",
    "            self.net.backward(pred_grad)\n",
    "            \n",
    "            \n",
    "            for i in range(len(self.layers_arch)):\n",
    "                if self.layers_arch[i][0] == 'Linear':\n",
    "                    self.net.layers[i].W -= self.net.layers[i].W_grad * self.learning_rate\n",
    "            ##########  Sample code ##########\n",
    "            \n",
    "        return loss/n_loop\n",
    "\n",
    "    def Test(self):\n",
    "        prediction = self.net.forward(self.test_data)\n",
    "        loss = self.loss_function.forward(prediction, self.test_label)\n",
    "        return loss\n",
    "\n",
    "    def Train(self):\n",
    "        self.dataloader()\n",
    "        for i in range(self.max_epoch):\n",
    "            train_loss = self.Train_One_Epoch()\n",
    "            test_loss = self.Test()\n",
    "            print(\"Epoch: \", str(i+1), \"/\", str(self.max_epoch), \" | Train loss: \", train_loss, \" | Test loss : \", test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 / 200  | Train loss:  0.617118547644125  | Test loss :  0.5622454921299812\n",
      "Epoch:  2 / 200  | Train loss:  0.5570564378052139  | Test loss :  0.5081067824169827\n",
      "Epoch:  3 / 200  | Train loss:  0.5039362906789453  | Test loss :  0.46032526774996413\n",
      "Epoch:  4 / 200  | Train loss:  0.45704310122310177  | Test loss :  0.4182367799009119\n",
      "Epoch:  5 / 200  | Train loss:  0.4171982939782158  | Test loss :  0.38550541601941546\n",
      "Epoch:  6 / 200  | Train loss:  0.3865412764103668  | Test loss :  0.3581048731495997\n",
      "Epoch:  7 / 200  | Train loss:  0.3592689588376993  | Test loss :  0.33343201029675185\n",
      "Epoch:  8 / 200  | Train loss:  0.33470062766859004  | Test loss :  0.31125785569558184\n",
      "Epoch:  9 / 200  | Train loss:  0.312609976957543  | Test loss :  0.2913602888825693\n",
      "Epoch:  10 / 200  | Train loss:  0.29277931975931765  | Test loss :  0.27354679632619194\n",
      "Epoch:  11 / 200  | Train loss:  0.2750093078071141  | Test loss :  0.257622529530386\n",
      "Epoch:  12 / 200  | Train loss:  0.25911216098847245  | Test loss :  0.2434137351281476\n",
      "Epoch:  13 / 200  | Train loss:  0.24491263990159298  | Test loss :  0.23075375799673473\n",
      "Epoch:  14 / 200  | Train loss:  0.23224812648455956  | Test loss :  0.219496289662139\n",
      "Epoch:  15 / 200  | Train loss:  0.2209697023791475  | Test loss :  0.20950011330524518\n",
      "Epoch:  16 / 200  | Train loss:  0.2109391623024598  | Test loss :  0.20063702751499027\n",
      "Epoch:  17 / 200  | Train loss:  0.20203084233429655  | Test loss :  0.19279039092671063\n",
      "Epoch:  18 / 200  | Train loss:  0.19412912493936005  | Test loss :  0.1858536356276764\n",
      "Epoch:  19 / 200  | Train loss:  0.1871287736409529  | Test loss :  0.17972987801150875\n",
      "Epoch:  20 / 200  | Train loss:  0.18093418324014596  | Test loss :  0.1743300287475328\n",
      "Epoch:  21 / 200  | Train loss:  0.1754584120171117  | Test loss :  0.16957647239312898\n",
      "Epoch:  22 / 200  | Train loss:  0.17062339711005886  | Test loss :  0.16539675443181984\n",
      "Epoch:  23 / 200  | Train loss:  0.1663582345305449  | Test loss :  0.16172571044171463\n",
      "Epoch:  24 / 200  | Train loss:  0.16259945139026175  | Test loss :  0.1585047396696417\n",
      "Epoch:  25 / 200  | Train loss:  0.15928949691453387  | Test loss :  0.1556832106708992\n",
      "Epoch:  26 / 200  | Train loss:  0.15637747010198896  | Test loss :  0.15321392545555138\n",
      "Epoch:  27 / 200  | Train loss:  0.15381756666964258  | Test loss :  0.15105543940989322\n",
      "Epoch:  28 / 200  | Train loss:  0.15156884232330922  | Test loss :  0.14917076334146329\n",
      "Epoch:  29 / 200  | Train loss:  0.1495948754163484  | Test loss :  0.14752703874611778\n",
      "Epoch:  30 / 200  | Train loss:  0.14786327219972506  | Test loss :  0.14609515625222275\n",
      "Epoch:  31 / 200  | Train loss:  0.14634512949796527  | Test loss :  0.14484906716294296\n",
      "Epoch:  32 / 200  | Train loss:  0.1450150083499771  | Test loss :  0.14376603608421445\n",
      "Epoch:  33 / 200  | Train loss:  0.14385022915058537  | Test loss :  0.14282578187583803\n",
      "Epoch:  34 / 200  | Train loss:  0.1428307732107898  | Test loss :  0.14201042766326516\n",
      "Epoch:  35 / 200  | Train loss:  0.14193895116894578  | Test loss :  0.14130429556307456\n",
      "Epoch:  36 / 200  | Train loss:  0.141159142176822  | Test loss :  0.1406935369059116\n",
      "Epoch:  37 / 200  | Train loss:  0.140477566542623  | Test loss :  0.14016582985283654\n",
      "Epoch:  38 / 200  | Train loss:  0.1398820954953579  | Test loss :  0.13971070665362706\n",
      "Epoch:  39 / 200  | Train loss:  0.13936201351302085  | Test loss :  0.1393186472235837\n",
      "Epoch:  40 / 200  | Train loss:  0.13890799906881968  | Test loss :  0.13898146848114631\n",
      "Epoch:  41 / 200  | Train loss:  0.13851176541888657  | Test loss :  0.13869200369450982\n",
      "Epoch:  42 / 200  | Train loss:  0.13816609044100628  | Test loss :  0.1384437980537557\n",
      "Epoch:  43 / 200  | Train loss:  0.137864505346667  | Test loss :  0.13823147080458376\n",
      "Epoch:  44 / 200  | Train loss:  0.13760157088243213  | Test loss :  0.13805024673543298\n",
      "Epoch:  45 / 200  | Train loss:  0.1373723802986891  | Test loss :  0.1378959025358941\n",
      "Epoch:  46 / 200  | Train loss:  0.13717261524349889  | Test loss :  0.1377647877809861\n",
      "Epoch:  47 / 200  | Train loss:  0.13699859099310038  | Test loss :  0.13765366845350047\n",
      "Epoch:  48 / 200  | Train loss:  0.13684693971936068  | Test loss :  0.13755982907777478\n",
      "Epoch:  49 / 200  | Train loss:  0.136714875934718  | Test loss :  0.13748087646843016\n",
      "Epoch:  50 / 200  | Train loss:  0.13659987941844978  | Test loss :  0.13741470537629316\n",
      "Epoch:  51 / 200  | Train loss:  0.13649975791873936  | Test loss :  0.13735950368773456\n",
      "Epoch:  52 / 200  | Train loss:  0.13641260386824217  | Test loss :  0.13731370213635052\n",
      "Epoch:  53 / 200  | Train loss:  0.1363367482484951  | Test loss :  0.13727594021672868\n",
      "Epoch:  54 / 200  | Train loss:  0.1362707368129466  | Test loss :  0.13724504130287193\n",
      "Epoch:  55 / 200  | Train loss:  0.1362133287422111  | Test loss :  0.13721999115448275\n",
      "Epoch:  56 / 200  | Train loss:  0.13616333071968884  | Test loss :  0.13719990270453522\n",
      "Epoch:  57 / 200  | Train loss:  0.1361198906172114  | Test loss :  0.1371840291534318\n",
      "Epoch:  58 / 200  | Train loss:  0.1360820560599236  | Test loss :  0.13717171181132778\n",
      "Epoch:  59 / 200  | Train loss:  0.13604920632996426  | Test loss :  0.1371623922852579\n",
      "Epoch:  60 / 200  | Train loss:  0.13602059349156675  | Test loss :  0.13715558878887094\n",
      "Epoch:  61 / 200  | Train loss:  0.135995737018065  | Test loss :  0.1371508821191515\n",
      "Epoch:  62 / 200  | Train loss:  0.13597412155294344  | Test loss :  0.13714792180666138\n",
      "Epoch:  63 / 200  | Train loss:  0.13595533163398085  | Test loss :  0.13714640488957777\n",
      "Epoch:  64 / 200  | Train loss:  0.13593903266928364  | Test loss :  0.1371460707450813\n",
      "Epoch:  65 / 200  | Train loss:  0.13592481923251648  | Test loss :  0.137146699546705\n",
      "Epoch:  66 / 200  | Train loss:  0.13591248173908652  | Test loss :  0.13714810416745307\n",
      "Epoch:  67 / 200  | Train loss:  0.13590176093830955  | Test loss :  0.1371501255176045\n",
      "Epoch:  68 / 200  | Train loss:  0.1358924457978979  | Test loss :  0.13715262907616851\n",
      "Epoch:  69 / 200  | Train loss:  0.13588435271744326  | Test loss :  0.13715550145415464\n",
      "Epoch:  70 / 200  | Train loss:  0.13587732198164995  | Test loss :  0.13715864741604286\n",
      "Epoch:  71 / 200  | Train loss:  0.13587121466870739  | Test loss :  0.13716198730020135\n",
      "Epoch:  72 / 200  | Train loss:  0.13586591179100732  | Test loss :  0.13716545330112068\n",
      "Epoch:  73 / 200  | Train loss:  0.13586130458723275  | Test loss :  0.1371689934595137\n",
      "Epoch:  74 / 200  | Train loss:  0.13585738130268785  | Test loss :  0.13717256349932405\n",
      "Epoch:  75 / 200  | Train loss:  0.13585382728475692  | Test loss :  0.13717611512868122\n",
      "Epoch:  76 / 200  | Train loss:  0.13585081552707212  | Test loss :  0.1371796344424068\n",
      "Epoch:  77 / 200  | Train loss:  0.1358481959135193  | Test loss :  0.1371830908376124\n",
      "Epoch:  78 / 200  | Train loss:  0.13584592182152433  | Test loss :  0.13718646556155736\n",
      "Epoch:  79 / 200  | Train loss:  0.13584394788002685  | Test loss :  0.13718974413850815\n",
      "Epoch:  80 / 200  | Train loss:  0.13584223465615103  | Test loss :  0.1371929156887972\n",
      "Epoch:  81 / 200  | Train loss:  0.13584074788046246  | Test loss :  0.13719597234465514\n",
      "Epoch:  82 / 200  | Train loss:  0.13583945777292417  | Test loss :  0.13719890874975035\n",
      "Epoch:  83 / 200  | Train loss:  0.13583833845653023  | Test loss :  0.13720172163108293\n",
      "Epoch:  84 / 200  | Train loss:  0.13583736744726554  | Test loss :  0.13720440943337425\n",
      "Epoch:  85 / 200  | Train loss:  0.13583652521050138  | Test loss :  0.13720697200739476\n",
      "Epoch:  86 / 200  | Train loss:  0.1358357947752094  | Test loss :  0.13720941034480547\n",
      "Epoch:  87 / 200  | Train loss:  0.13583516139848753  | Test loss :  0.1372117263530752\n",
      "Epoch:  88 / 200  | Train loss:  0.1358346122738607  | Test loss :  0.1372139226648951\n",
      "Epoch:  89 / 200  | Train loss:  0.13583413627766486  | Test loss :  0.13721600247725688\n",
      "Epoch:  90 / 200  | Train loss:  0.13583372374855834  | Test loss :  0.13721796941601103\n",
      "Epoch:  91 / 200  | Train loss:  0.13583336629584702  | Test loss :  0.13721982742228586\n",
      "Epoch:  92 / 200  | Train loss:  0.1358330566328692  | Test loss :  0.1372215806576368\n",
      "Epoch:  93 / 200  | Train loss:  0.13583278843217378  | Test loss :  0.13722323342522139\n",
      "Epoch:  94 / 200  | Train loss:  0.13583255619964726  | Test loss :  0.13722479010466507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  95 / 200  | Train loss:  0.13583242262048614  | Test loss :  0.1372260231758001\n",
      "Epoch:  96 / 200  | Train loss:  0.13583232633524617  | Test loss :  0.13722765014647265\n",
      "Epoch:  97 / 200  | Train loss:  0.13583202885417392  | Test loss :  0.13722894380927067\n",
      "Epoch:  98 / 200  | Train loss:  0.13583189892491215  | Test loss :  0.137230158792741\n",
      "Epoch:  99 / 200  | Train loss:  0.13583178659549253  | Test loss :  0.13723129923298072\n",
      "Epoch:  100 / 200  | Train loss:  0.13583168951964725  | Test loss :  0.13723236914369433\n",
      "Epoch:  101 / 200  | Train loss:  0.13583160566133884  | Test loss :  0.13723337240372632\n",
      "Epoch:  102 / 200  | Train loss:  0.13583153325392497  | Test loss :  0.13723431274816048\n",
      "Epoch:  103 / 200  | Train loss:  0.13583147076468283  | Test loss :  0.13723519376238882\n",
      "Epoch:  104 / 200  | Train loss:  0.13583141686399258  | Test loss :  0.13723601887863876\n",
      "Epoch:  105 / 200  | Train loss:  0.13583137039856755  | Test loss :  0.13723679137452302\n",
      "Epoch:  106 / 200  | Train loss:  0.1358313303682022  | Test loss :  0.13723751437324105\n",
      "Epoch:  107 / 200  | Train loss:  0.13583129590557602  | Test loss :  0.13723819084511682\n",
      "Epoch:  108 / 200  | Train loss:  0.13583126625871303  | Test loss :  0.13723882361020473\n",
      "Epoch:  109 / 200  | Train loss:  0.13583124077574765  | Test loss :  0.13723941534173878\n",
      "Epoch:  110 / 200  | Train loss:  0.1358312188916955  | Test loss :  0.13723996857023318\n",
      "Epoch:  111 / 200  | Train loss:  0.13583120011696503  | Test loss :  0.13724048568807456\n",
      "Epoch:  112 / 200  | Train loss:  0.13583118402738195  | Test loss :  0.1372409689544713\n",
      "Epoch:  113 / 200  | Train loss:  0.1358311702555274  | Test loss :  0.1372414205006483\n",
      "Epoch:  114 / 200  | Train loss:  0.13583115848321756  | Test loss :  0.13724184233519415\n",
      "Epoch:  115 / 200  | Train loss:  0.1358311484349749  | Test loss :  0.13724223634948404\n",
      "Epoch:  116 / 200  | Train loss:  0.13583113987235995  | Test loss :  0.1372426043231162\n",
      "Epoch:  117 / 200  | Train loss:  0.1358311325890515  | Test loss :  0.13724294792930977\n",
      "Epoch:  118 / 200  | Train loss:  0.1358311264065758  | Test loss :  0.13724326874022508\n",
      "Epoch:  119 / 200  | Train loss:  0.13583112117060017  | Test loss :  0.13724356823217185\n",
      "Epoch:  120 / 200  | Train loss:  0.13583111674771642  | Test loss :  0.13724384779068163\n",
      "Epoch:  121 / 200  | Train loss:  0.13583111302264914  | Test loss :  0.13724410871542436\n",
      "Epoch:  122 / 200  | Train loss:  0.13583110989583413  | Test loss :  0.13724435222495512\n",
      "Epoch:  123 / 200  | Train loss:  0.1358311072813168  | Test loss :  0.13724457946128119\n",
      "Epoch:  124 / 200  | Train loss:  0.1358311051049291  | Test loss :  0.13724479149424226\n",
      "Epoch:  125 / 200  | Train loss:  0.13583110330270864  | Test loss :  0.13724498932570145\n",
      "Epoch:  126 / 200  | Train loss:  0.13583110181952707  | Test loss :  0.13724517389354463\n",
      "Epoch:  127 / 200  | Train loss:  0.1358311006079008  | Test loss :  0.1372453460754895\n",
      "Epoch:  128 / 200  | Train loss:  0.13583109962695997  | Test loss :  0.13724550669270605\n",
      "Epoch:  129 / 200  | Train loss:  0.1358310988415544  | Test loss :  0.1372456565132535\n",
      "Epoch:  130 / 200  | Train loss:  0.1358310982214788  | Test loss :  0.13724579625533545\n",
      "Epoch:  131 / 200  | Train loss:  0.13583109774080107  | Test loss :  0.1372459265903817\n",
      "Epoch:  132 / 200  | Train loss:  0.1358310973772806  | Test loss :  0.1372460481459597\n",
      "Epoch:  133 / 200  | Train loss:  0.13583109711186428  | Test loss :  0.13724616150852392\n",
      "Epoch:  134 / 200  | Train loss:  0.13583109692824952  | Test loss :  0.13724626722600836\n",
      "Epoch:  135 / 200  | Train loss:  0.13583109681250666  | Test loss :  0.13724636581026942\n",
      "Epoch:  136 / 200  | Train loss:  0.13583109675275143  | Test loss :  0.1372464577393858\n",
      "Epoch:  137 / 200  | Train loss:  0.13583109673886173  | Test loss :  0.13724654345982185\n",
      "Epoch:  138 / 200  | Train loss:  0.1358310967622328  | Test loss :  0.1372466233884613\n",
      "Epoch:  139 / 200  | Train loss:  0.13583109681556474  | Test loss :  0.13724669791451788\n",
      "Epoch:  140 / 200  | Train loss:  0.13583109689267991  | Test loss :  0.13724676740132913\n",
      "Epoch:  141 / 200  | Train loss:  0.13583109698836407  | Test loss :  0.1372468321880396\n",
      "Epoch:  142 / 200  | Train loss:  0.13583109709822946  | Test loss :  0.13724689259117973\n",
      "Epoch:  143 / 200  | Train loss:  0.13583109721859726  | Test loss :  0.1372469489061457\n",
      "Epoch:  144 / 200  | Train loss:  0.13583109734639476  | Test loss :  0.13724700140858634\n",
      "Epoch:  145 / 200  | Train loss:  0.1358310974790679  | Test loss :  0.1372470503557025\n",
      "Epoch:  146 / 200  | Train loss:  0.13583109761450507  | Test loss :  0.13724709598746312\n",
      "Epoch:  147 / 200  | Train loss:  0.13583109775097194  | Test loss :  0.13724713852774412\n",
      "Epoch:  148 / 200  | Train loss:  0.1358310978870551  | Test loss :  0.137247178185394\n",
      "Epoch:  149 / 200  | Train loss:  0.13583109802161353  | Test loss :  0.13724721515523047\n",
      "Epoch:  150 / 200  | Train loss:  0.1358310981537369  | Test loss :  0.1372472496189727\n",
      "Epoch:  151 / 200  | Train loss:  0.13583109828270995  | Test loss :  0.13724728174611325\n",
      "Epoch:  152 / 200  | Train loss:  0.13583109840798158  | Test loss :  0.13724731169473267\n",
      "Epoch:  153 / 200  | Train loss:  0.13583109852913866  | Test loss :  0.13724733961226138\n",
      "Epoch:  154 / 200  | Train loss:  0.1358310986458833  | Test loss :  0.13724736563619097\n",
      "Epoch:  155 / 200  | Train loss:  0.13583109875801377  | Test loss :  0.13724738989473947\n",
      "Epoch:  156 / 200  | Train loss:  0.13583109886540784  | Test loss :  0.13724741250747222\n",
      "Epoch:  157 / 200  | Train loss:  0.13583109896800877  | Test loss :  0.13724743358588196\n",
      "Epoch:  158 / 200  | Train loss:  0.13583109906581353  | Test loss :  0.1372474532339305\n",
      "Epoch:  159 / 200  | Train loss:  0.13583109915886216  | Test loss :  0.13724747154855468\n",
      "Epoch:  160 / 200  | Train loss:  0.1358310992472298  | Test loss :  0.13724748862013844\n",
      "Epoch:  161 / 200  | Train loss:  0.13583109933101897  | Test loss :  0.13724750453295342\n",
      "Epoch:  162 / 200  | Train loss:  0.13583109941035346  | Test loss :  0.1372475193655708\n",
      "Epoch:  163 / 200  | Train loss:  0.1358310994853733  | Test loss :  0.13724753319124494\n",
      "Epoch:  164 / 200  | Train loss:  0.13583109955623027  | Test loss :  0.13724754607827186\n",
      "Epoch:  165 / 200  | Train loss:  0.1358310996230845  | Test loss :  0.13724755809032366\n",
      "Epoch:  166 / 200  | Train loss:  0.1358310996861014  | Test loss :  0.13724756928676032\n",
      "Epoch:  167 / 200  | Train loss:  0.13583109974544902  | Test loss :  0.1372475797229211\n",
      "Epoch:  168 / 200  | Train loss:  0.1358310998012962  | Test loss :  0.13724758945039564\n",
      "Epoch:  169 / 200  | Train loss:  0.13583109985381084  | Test loss :  0.13724759851727775\n",
      "Epoch:  170 / 200  | Train loss:  0.13583109990315873  | Test loss :  0.13724760696840144\n",
      "Epoch:  171 / 200  | Train loss:  0.13583109994950218  | Test loss :  0.13724761484556125\n",
      "Epoch:  172 / 200  | Train loss:  0.13583109999299964  | Test loss :  0.13724762218771822\n",
      "Epoch:  173 / 200  | Train loss:  0.1358311000338046  | Test loss :  0.13724762903119112\n",
      "Epoch:  174 / 200  | Train loss:  0.13583110007206545  | Test loss :  0.13724763540983562\n",
      "Epoch:  175 / 200  | Train loss:  0.13583110010792504  | Test loss :  0.137247641355211\n",
      "Epoch:  176 / 200  | Train loss:  0.13583110014152036  | Test loss :  0.1372476468967355\n",
      "Epoch:  177 / 200  | Train loss:  0.13583110017298267  | Test loss :  0.1372476520618314\n",
      "Epoch:  178 / 200  | Train loss:  0.13583110020243713  | Test loss :  0.13724765687606033\n",
      "Epoch:  179 / 200  | Train loss:  0.1358311002300032  | Test loss :  0.13724766136324928\n",
      "Epoch:  180 / 200  | Train loss:  0.13583110025579423  | Test loss :  0.13724766554560808\n",
      "Epoch:  181 / 200  | Train loss:  0.13583110027991793  | Test loss :  0.13724766944383923\n",
      "Epoch:  182 / 200  | Train loss:  0.13583110030247642  | Test loss :  0.1372476730772399\n",
      "Epoch:  183 / 200  | Train loss:  0.13583110032356627  | Test loss :  0.1372476764637973\n",
      "Epoch:  184 / 200  | Train loss:  0.1358311003432788  | Test loss :  0.13724767962027762\n",
      "Epoch:  185 / 200  | Train loss:  0.13583110036170032  | Test loss :  0.13724768256230857\n",
      "Epoch:  186 / 200  | Train loss:  0.13583110037891208  | Test loss :  0.13724768530445694\n",
      "Epoch:  187 / 200  | Train loss:  0.13583110039499086  | Test loss :  0.13724768786030028\n",
      "Epoch:  188 / 200  | Train loss:  0.13583110041000862  | Test loss :  0.13724769024249434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  189 / 200  | Train loss:  0.13583110042403357  | Test loss :  0.13724769246283533\n",
      "Epoch:  190 / 200  | Train loss:  0.13583110043712923  | Test loss :  0.13724769453231833\n",
      "Epoch:  191 / 200  | Train loss:  0.13583110044935578  | Test loss :  0.13724769646119184\n",
      "Epoch:  192 / 200  | Train loss:  0.13583110046076946  | Test loss :  0.13724769825900823\n",
      "Epoch:  193 / 200  | Train loss:  0.1358311004714231  | Test loss :  0.1372476999346711\n",
      "Epoch:  194 / 200  | Train loss:  0.13583110048136637  | Test loss :  0.1372477014964793\n",
      "Epoch:  195 / 200  | Train loss:  0.13583110049064565  | Test loss :  0.137247702952168\n",
      "Epoch:  196 / 200  | Train loss:  0.13583110049930458  | Test loss :  0.1372477043089469\n",
      "Epoch:  197 / 200  | Train loss:  0.13583110050738384  | Test loss :  0.13724770557353616\n",
      "Epoch:  198 / 200  | Train loss:  0.13583110051492173  | Test loss :  0.13724770675219916\n",
      "Epoch:  199 / 200  | Train loss:  0.13583110052195396  | Test loss :  0.137247707850774\n",
      "Epoch:  200 / 200  | Train loss:  0.13583110052851408  | Test loss :  0.13724770887470217\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.13724770887470217"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_path = './MNIST_Sub/train_data.txt'\n",
    "train_labels_path = './MNIST_Sub/train_labels.txt'\n",
    "test_data_path = './MNIST_Sub/test_data.txt'\n",
    "test_labels_path = './MNIST_Sub/test_labels.txt'\n",
    "\n",
    "# regressor\n",
    "regressor_layers_arch = [['Linear', (1, 16)], ['ReLU'], ['Linear', (16, 16)], ['ReLU'], ['Linear', (16, 1)]]\n",
    "def data_function(x):\n",
    "    return np.power(x,3) + pow(x,2) + 1\n",
    "regressor = Regressor(regressor_layers_arch, data_function, learning_rate = 1e-4, batch_size = 32, max_epoch = 200)\n",
    "regressor.Train()\n",
    "\n",
    "regressor.Test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Classfication Network\n",
    "Implement your own classifier with gradient descent. Please read the requirement for Task 5 in the pdf document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def One_Hot_Encode(labels, classes = 10):\n",
    "    '''\n",
    "    /*  Make the labels one-hot.\n",
    "     *  For example, if there are 5 classes {0, 1, 2, 3, 4} then\n",
    "     *  [0, 2, 4] -> [[1, 0, 0, 0, 0],\n",
    "     * \t\t\t\t\t\t\t\t[0, 0, 1, 0, 0],\n",
    "     * \t\t\t\t\t\t\t\t[0, 0, 0, 0, 1]]\n",
    "     */\n",
    "    '''\n",
    "    \n",
    "    ########## Code start  ##########\n",
    "    \n",
    "    class_matrix = np.zeros((len(labels), classes))\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        class_matrix[i][labels[i]] = 1\n",
    "    \n",
    "    return class_matrix\n",
    "    \n",
    "    ##########  Code end   ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    #Classifier\n",
    "    def __init__(self, train_data_path, train_labels_path, test_data_path, test_labels_path, layers_arch, learning_rate = 1e-3, batch_size = 32, max_epoch = 200, classes = 10):\n",
    "        self.classes = classes\n",
    "\n",
    "        self.train_data_path = train_data_path\n",
    "        self.train_labels_path = train_labels_path\n",
    "        self.test_data_path = test_data_path\n",
    "        self.test_labels_path = test_labels_path\n",
    "\n",
    "\n",
    "        self.train_data = [] #The shape of train data should be (n_samples,28^2)\n",
    "        self.train_labels = []\n",
    "        self.test_data = []\n",
    "        self.test_labels = []\n",
    "        self.pepochs = []\n",
    "        self.ploss = []\n",
    "        self.pacc = []\n",
    "        \n",
    "        self.layers_arch = layers_arch\n",
    "        self.net = Network(layers_arch)\n",
    "        self.loss_function = MSELoss()\n",
    "\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "\n",
    "    def dataloader(self):\n",
    "\n",
    "        with open(self.train_data_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                self.train_data.append(np.array(line.strip().split()).astype(np.float64)/255.0)\n",
    "        self.train_data = np.array(self.train_data)\n",
    "\n",
    "        with open(self.train_labels_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                self.train_labels.append(int(line.strip()))\n",
    "        self.train_labels = np.array(self.train_labels)\n",
    "\n",
    "        with open(self.test_data_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                self.test_data.append(np.array(line.strip().split()).astype(np.float64)/255.0)\n",
    "        self.test_data = np.array(self.test_data)\n",
    "\n",
    "        with open(self.test_labels_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                self.test_labels.append(int(line.strip()))\n",
    "        self.test_labels = np.array(self.test_labels)\n",
    "\n",
    "\n",
    "    def Train_One_Epoch(self):\n",
    "        '''\n",
    "        Here we train the network using gradient descent\n",
    "        '''\n",
    "        loss = 0\n",
    "        n_loop = int(math.ceil(len(self.train_data) / self.batch_size))\n",
    "        for i in range(n_loop):\n",
    "            batch_data = self.train_data[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "            batch_label = self.train_labels[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "            batch_one_hot_label = One_Hot_Encode(batch_label, classes = self.classes)\n",
    "            \n",
    "            '''\n",
    "             /*  Forward the data to the network.\n",
    "             *  Forward the result to the loss function.\n",
    "             *  Backward.\n",
    "             *  Update the weights with weight gradients.\n",
    "             *  Do not forget the learning rate!\n",
    "             */\n",
    "            '''\n",
    "            \n",
    "            ########## Code start  ##########\n",
    "            prediction = self.net.forward(batch_data)\n",
    "            loss += self.loss_function.forward(prediction, batch_one_hot_label)\n",
    "\n",
    "            pred_grad = self.loss_function.backward()\n",
    "            self.net.backward(pred_grad)\n",
    "            \n",
    "            \n",
    "            for i in range(len(self.layers_arch)):\n",
    "                if self.layers_arch[i][0] == 'Linear':\n",
    "                    self.net.layers[i].W -= self.net.layers[i].W_grad * self.learning_rate\n",
    "            ##########  Code end   ##########\n",
    "        \n",
    "        return loss / n_loop\n",
    "\n",
    "    def Test(self):\n",
    "        '''\n",
    "        the class with max score is our predicted label\n",
    "        '''\n",
    "        score = self.net.forward(self.test_data)\n",
    "        accuracy = 0\n",
    "        for i in range(np.shape(score)[0]):\n",
    "            one_label_list = score[i].tolist()\n",
    "            label_pred = one_label_list.index(max(one_label_list))\n",
    "            if label_pred == self.test_labels[i]:\n",
    "                accuracy = accuracy +1\n",
    "\n",
    "        accuracy = accuracy/np.shape(score)[0]\n",
    "        return accuracy\n",
    "\n",
    "    def Train(self):\n",
    "        self.dataloader()\n",
    "        for i in range(self.max_epoch):\n",
    "            loss = self.Train_One_Epoch()\n",
    "            accuray = self.Test()\n",
    "            print(\"Epoch: \", str(i+1), \"/\", str(self.max_epoch), \" | Train loss: \", loss, \" | Test Accuracy : \", accuray)\n",
    "            self.ploss.append(loss)\n",
    "            self.pacc.append(accuray)\n",
    "            self.pepochs.append(i+1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "That's it! Congratulations on finishing everything. Now try your network on MNIST!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 / 200  | Train loss:  1.2032403939693412  | Test Accuracy :  0.435\n",
      "Epoch:  2 / 200  | Train loss:  0.7511231772382598  | Test Accuracy :  0.58\n",
      "Epoch:  3 / 200  | Train loss:  0.6185338925466222  | Test Accuracy :  0.69\n",
      "Epoch:  4 / 200  | Train loss:  0.543943894166101  | Test Accuracy :  0.725\n",
      "Epoch:  5 / 200  | Train loss:  0.49357159517196036  | Test Accuracy :  0.75\n",
      "Epoch:  6 / 200  | Train loss:  0.4558785678693509  | Test Accuracy :  0.785\n",
      "Epoch:  7 / 200  | Train loss:  0.42597768662338614  | Test Accuracy :  0.8\n",
      "Epoch:  8 / 200  | Train loss:  0.4013062958259187  | Test Accuracy :  0.805\n",
      "Epoch:  9 / 200  | Train loss:  0.38044154007356196  | Test Accuracy :  0.81\n",
      "Epoch:  10 / 200  | Train loss:  0.36238221124972964  | Test Accuracy :  0.805\n",
      "Epoch:  11 / 200  | Train loss:  0.3465081594321483  | Test Accuracy :  0.81\n",
      "Epoch:  12 / 200  | Train loss:  0.33223648822561963  | Test Accuracy :  0.82\n",
      "Epoch:  13 / 200  | Train loss:  0.31940474664484386  | Test Accuracy :  0.82\n",
      "Epoch:  14 / 200  | Train loss:  0.30764299094492353  | Test Accuracy :  0.82\n",
      "Epoch:  15 / 200  | Train loss:  0.29689134406729223  | Test Accuracy :  0.82\n",
      "Epoch:  16 / 200  | Train loss:  0.2869064422713618  | Test Accuracy :  0.82\n",
      "Epoch:  17 / 200  | Train loss:  0.27768215700960464  | Test Accuracy :  0.825\n",
      "Epoch:  18 / 200  | Train loss:  0.2690910782671333  | Test Accuracy :  0.835\n",
      "Epoch:  19 / 200  | Train loss:  0.2610704085350784  | Test Accuracy :  0.845\n",
      "Epoch:  20 / 200  | Train loss:  0.2535717369334196  | Test Accuracy :  0.85\n",
      "Epoch:  21 / 200  | Train loss:  0.24647910314748533  | Test Accuracy :  0.855\n",
      "Epoch:  22 / 200  | Train loss:  0.23985538521628005  | Test Accuracy :  0.855\n",
      "Epoch:  23 / 200  | Train loss:  0.23356999211306192  | Test Accuracy :  0.855\n",
      "Epoch:  24 / 200  | Train loss:  0.22766253667037806  | Test Accuracy :  0.855\n",
      "Epoch:  25 / 200  | Train loss:  0.22203776867461683  | Test Accuracy :  0.855\n",
      "Epoch:  26 / 200  | Train loss:  0.2166989615207931  | Test Accuracy :  0.855\n",
      "Epoch:  27 / 200  | Train loss:  0.21160678609302813  | Test Accuracy :  0.86\n",
      "Epoch:  28 / 200  | Train loss:  0.206712647716294  | Test Accuracy :  0.86\n",
      "Epoch:  29 / 200  | Train loss:  0.2020944398686461  | Test Accuracy :  0.86\n",
      "Epoch:  30 / 200  | Train loss:  0.1976371755296471  | Test Accuracy :  0.86\n",
      "Epoch:  31 / 200  | Train loss:  0.19335331392246535  | Test Accuracy :  0.86\n",
      "Epoch:  32 / 200  | Train loss:  0.18926015444260608  | Test Accuracy :  0.855\n",
      "Epoch:  33 / 200  | Train loss:  0.18535995147885936  | Test Accuracy :  0.855\n",
      "Epoch:  34 / 200  | Train loss:  0.18160852802523164  | Test Accuracy :  0.855\n",
      "Epoch:  35 / 200  | Train loss:  0.17800734182520583  | Test Accuracy :  0.855\n",
      "Epoch:  36 / 200  | Train loss:  0.17449074770255174  | Test Accuracy :  0.855\n",
      "Epoch:  37 / 200  | Train loss:  0.17111515622199752  | Test Accuracy :  0.855\n",
      "Epoch:  38 / 200  | Train loss:  0.16784548290615747  | Test Accuracy :  0.855\n",
      "Epoch:  39 / 200  | Train loss:  0.16471118343395025  | Test Accuracy :  0.85\n",
      "Epoch:  40 / 200  | Train loss:  0.16167208770933114  | Test Accuracy :  0.85\n",
      "Epoch:  41 / 200  | Train loss:  0.15871900465224764  | Test Accuracy :  0.85\n",
      "Epoch:  42 / 200  | Train loss:  0.15586864661220556  | Test Accuracy :  0.85\n",
      "Epoch:  43 / 200  | Train loss:  0.15312561151748103  | Test Accuracy :  0.85\n",
      "Epoch:  44 / 200  | Train loss:  0.15048150047698008  | Test Accuracy :  0.85\n",
      "Epoch:  45 / 200  | Train loss:  0.1478794840042625  | Test Accuracy :  0.855\n",
      "Epoch:  46 / 200  | Train loss:  0.14539869383216805  | Test Accuracy :  0.86\n",
      "Epoch:  47 / 200  | Train loss:  0.14296021955558372  | Test Accuracy :  0.865\n",
      "Epoch:  48 / 200  | Train loss:  0.14060795752417268  | Test Accuracy :  0.865\n",
      "Epoch:  49 / 200  | Train loss:  0.1383195857164207  | Test Accuracy :  0.865\n",
      "Epoch:  50 / 200  | Train loss:  0.13609927359657775  | Test Accuracy :  0.865\n",
      "Epoch:  51 / 200  | Train loss:  0.1339148117361781  | Test Accuracy :  0.865\n",
      "Epoch:  52 / 200  | Train loss:  0.13183093844605712  | Test Accuracy :  0.865\n",
      "Epoch:  53 / 200  | Train loss:  0.12978304480323097  | Test Accuracy :  0.865\n",
      "Epoch:  54 / 200  | Train loss:  0.12780954116975213  | Test Accuracy :  0.865\n",
      "Epoch:  55 / 200  | Train loss:  0.12584998094179767  | Test Accuracy :  0.865\n",
      "Epoch:  56 / 200  | Train loss:  0.12395997280244211  | Test Accuracy :  0.865\n",
      "Epoch:  57 / 200  | Train loss:  0.12212551870377081  | Test Accuracy :  0.865\n",
      "Epoch:  58 / 200  | Train loss:  0.12030223752011797  | Test Accuracy :  0.865\n",
      "Epoch:  59 / 200  | Train loss:  0.1185378743519009  | Test Accuracy :  0.865\n",
      "Epoch:  60 / 200  | Train loss:  0.1168350463166559  | Test Accuracy :  0.865\n",
      "Epoch:  61 / 200  | Train loss:  0.11515103928935014  | Test Accuracy :  0.865\n",
      "Epoch:  62 / 200  | Train loss:  0.11350506167575752  | Test Accuracy :  0.865\n",
      "Epoch:  63 / 200  | Train loss:  0.11188525894084213  | Test Accuracy :  0.87\n",
      "Epoch:  64 / 200  | Train loss:  0.11033795045133854  | Test Accuracy :  0.87\n",
      "Epoch:  65 / 200  | Train loss:  0.10880482041812614  | Test Accuracy :  0.87\n",
      "Epoch:  66 / 200  | Train loss:  0.10730447483425597  | Test Accuracy :  0.865\n",
      "Epoch:  67 / 200  | Train loss:  0.10584973240134683  | Test Accuracy :  0.865\n",
      "Epoch:  68 / 200  | Train loss:  0.10440893819568497  | Test Accuracy :  0.865\n",
      "Epoch:  69 / 200  | Train loss:  0.10300581551608581  | Test Accuracy :  0.865\n",
      "Epoch:  70 / 200  | Train loss:  0.10163691085701637  | Test Accuracy :  0.865\n",
      "Epoch:  71 / 200  | Train loss:  0.10030361690490082  | Test Accuracy :  0.865\n",
      "Epoch:  72 / 200  | Train loss:  0.09899905141362304  | Test Accuracy :  0.865\n",
      "Epoch:  73 / 200  | Train loss:  0.0977099828495241  | Test Accuracy :  0.865\n",
      "Epoch:  74 / 200  | Train loss:  0.09645518394143655  | Test Accuracy :  0.87\n",
      "Epoch:  75 / 200  | Train loss:  0.09521532475057165  | Test Accuracy :  0.87\n",
      "Epoch:  76 / 200  | Train loss:  0.09401227522792989  | Test Accuracy :  0.87\n",
      "Epoch:  77 / 200  | Train loss:  0.0928341494469114  | Test Accuracy :  0.865\n",
      "Epoch:  78 / 200  | Train loss:  0.09168528162517066  | Test Accuracy :  0.87\n",
      "Epoch:  79 / 200  | Train loss:  0.09055333402183957  | Test Accuracy :  0.865\n",
      "Epoch:  80 / 200  | Train loss:  0.08944112704147623  | Test Accuracy :  0.87\n",
      "Epoch:  81 / 200  | Train loss:  0.08833990292078096  | Test Accuracy :  0.865\n",
      "Epoch:  82 / 200  | Train loss:  0.0872700876181057  | Test Accuracy :  0.865\n",
      "Epoch:  83 / 200  | Train loss:  0.08623484533321911  | Test Accuracy :  0.865\n",
      "Epoch:  84 / 200  | Train loss:  0.08521570303308483  | Test Accuracy :  0.865\n",
      "Epoch:  85 / 200  | Train loss:  0.0841905502730611  | Test Accuracy :  0.865\n",
      "Epoch:  86 / 200  | Train loss:  0.08319872972005886  | Test Accuracy :  0.865\n",
      "Epoch:  87 / 200  | Train loss:  0.0822367813953828  | Test Accuracy :  0.865\n",
      "Epoch:  88 / 200  | Train loss:  0.08126186716496471  | Test Accuracy :  0.865\n",
      "Epoch:  89 / 200  | Train loss:  0.08032914144668407  | Test Accuracy :  0.865\n",
      "Epoch:  90 / 200  | Train loss:  0.07940992706996501  | Test Accuracy :  0.865\n",
      "Epoch:  91 / 200  | Train loss:  0.07849038270281396  | Test Accuracy :  0.865\n",
      "Epoch:  92 / 200  | Train loss:  0.07760231646943325  | Test Accuracy :  0.865\n",
      "Epoch:  93 / 200  | Train loss:  0.07671344463701671  | Test Accuracy :  0.865\n",
      "Epoch:  94 / 200  | Train loss:  0.07583829821173158  | Test Accuracy :  0.865\n",
      "Epoch:  95 / 200  | Train loss:  0.0750025647548573  | Test Accuracy :  0.865\n",
      "Epoch:  96 / 200  | Train loss:  0.07416655192569077  | Test Accuracy :  0.865\n",
      "Epoch:  97 / 200  | Train loss:  0.07333593574051245  | Test Accuracy :  0.87\n",
      "Epoch:  98 / 200  | Train loss:  0.0725225796564442  | Test Accuracy :  0.87\n",
      "Epoch:  99 / 200  | Train loss:  0.07173905638958396  | Test Accuracy :  0.87\n",
      "Epoch:  100 / 200  | Train loss:  0.0709519465253666  | Test Accuracy :  0.875\n",
      "Epoch:  101 / 200  | Train loss:  0.0701916392411222  | Test Accuracy :  0.875\n",
      "Epoch:  102 / 200  | Train loss:  0.06943615666813199  | Test Accuracy :  0.875\n",
      "Epoch:  103 / 200  | Train loss:  0.06868859201105869  | Test Accuracy :  0.875\n",
      "Epoch:  104 / 200  | Train loss:  0.06797219054955222  | Test Accuracy :  0.875\n",
      "Epoch:  105 / 200  | Train loss:  0.06724718082930431  | Test Accuracy :  0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  106 / 200  | Train loss:  0.06653471489236408  | Test Accuracy :  0.875\n",
      "Epoch:  107 / 200  | Train loss:  0.065845585578282  | Test Accuracy :  0.875\n",
      "Epoch:  108 / 200  | Train loss:  0.06516112469582522  | Test Accuracy :  0.875\n",
      "Epoch:  109 / 200  | Train loss:  0.06448463190344399  | Test Accuracy :  0.875\n",
      "Epoch:  110 / 200  | Train loss:  0.06381731565541303  | Test Accuracy :  0.875\n",
      "Epoch:  111 / 200  | Train loss:  0.06316574366824095  | Test Accuracy :  0.875\n",
      "Epoch:  112 / 200  | Train loss:  0.06252913441191339  | Test Accuracy :  0.875\n",
      "Epoch:  113 / 200  | Train loss:  0.06189546260058111  | Test Accuracy :  0.875\n",
      "Epoch:  114 / 200  | Train loss:  0.06126132153132238  | Test Accuracy :  0.875\n",
      "Epoch:  115 / 200  | Train loss:  0.06064335208417447  | Test Accuracy :  0.875\n",
      "Epoch:  116 / 200  | Train loss:  0.06004343069210016  | Test Accuracy :  0.875\n",
      "Epoch:  117 / 200  | Train loss:  0.05944561267311772  | Test Accuracy :  0.875\n",
      "Epoch:  118 / 200  | Train loss:  0.05885395518238472  | Test Accuracy :  0.875\n",
      "Epoch:  119 / 200  | Train loss:  0.058281257019995696  | Test Accuracy :  0.875\n",
      "Epoch:  120 / 200  | Train loss:  0.0577010451308618  | Test Accuracy :  0.875\n",
      "Epoch:  121 / 200  | Train loss:  0.057136756276176634  | Test Accuracy :  0.875\n",
      "Epoch:  122 / 200  | Train loss:  0.056586139467468946  | Test Accuracy :  0.875\n",
      "Epoch:  123 / 200  | Train loss:  0.056037874718694966  | Test Accuracy :  0.875\n",
      "Epoch:  124 / 200  | Train loss:  0.055490958481347795  | Test Accuracy :  0.875\n",
      "Epoch:  125 / 200  | Train loss:  0.054960856833411856  | Test Accuracy :  0.875\n",
      "Epoch:  126 / 200  | Train loss:  0.05443826397677451  | Test Accuracy :  0.875\n",
      "Epoch:  127 / 200  | Train loss:  0.05391331962359846  | Test Accuracy :  0.875\n",
      "Epoch:  128 / 200  | Train loss:  0.05340137165336332  | Test Accuracy :  0.875\n",
      "Epoch:  129 / 200  | Train loss:  0.052893820472011306  | Test Accuracy :  0.875\n",
      "Epoch:  130 / 200  | Train loss:  0.05239663390512464  | Test Accuracy :  0.875\n",
      "Epoch:  131 / 200  | Train loss:  0.05190422876668707  | Test Accuracy :  0.875\n",
      "Epoch:  132 / 200  | Train loss:  0.05141644715733765  | Test Accuracy :  0.875\n",
      "Epoch:  133 / 200  | Train loss:  0.05093550523174944  | Test Accuracy :  0.875\n",
      "Epoch:  134 / 200  | Train loss:  0.050463067809694055  | Test Accuracy :  0.875\n",
      "Epoch:  135 / 200  | Train loss:  0.0499936310168162  | Test Accuracy :  0.875\n",
      "Epoch:  136 / 200  | Train loss:  0.0495342702019441  | Test Accuracy :  0.875\n",
      "Epoch:  137 / 200  | Train loss:  0.04908059915683017  | Test Accuracy :  0.875\n",
      "Epoch:  138 / 200  | Train loss:  0.04862549968483204  | Test Accuracy :  0.875\n",
      "Epoch:  139 / 200  | Train loss:  0.04818372299977876  | Test Accuracy :  0.875\n",
      "Epoch:  140 / 200  | Train loss:  0.04775159269244393  | Test Accuracy :  0.875\n",
      "Epoch:  141 / 200  | Train loss:  0.047313245222324926  | Test Accuracy :  0.875\n",
      "Epoch:  142 / 200  | Train loss:  0.046882653283231886  | Test Accuracy :  0.875\n",
      "Epoch:  143 / 200  | Train loss:  0.04645476540419059  | Test Accuracy :  0.875\n",
      "Epoch:  144 / 200  | Train loss:  0.04604379096153592  | Test Accuracy :  0.875\n",
      "Epoch:  145 / 200  | Train loss:  0.04562915254216444  | Test Accuracy :  0.875\n",
      "Epoch:  146 / 200  | Train loss:  0.04522011721329485  | Test Accuracy :  0.875\n",
      "Epoch:  147 / 200  | Train loss:  0.04481498616798417  | Test Accuracy :  0.875\n",
      "Epoch:  148 / 200  | Train loss:  0.044427945578152106  | Test Accuracy :  0.87\n",
      "Epoch:  149 / 200  | Train loss:  0.04402337702718911  | Test Accuracy :  0.865\n",
      "Epoch:  150 / 200  | Train loss:  0.04363524982632535  | Test Accuracy :  0.865\n",
      "Epoch:  151 / 200  | Train loss:  0.04326539875622512  | Test Accuracy :  0.865\n",
      "Epoch:  152 / 200  | Train loss:  0.042879032612876615  | Test Accuracy :  0.865\n",
      "Epoch:  153 / 200  | Train loss:  0.04250578423673591  | Test Accuracy :  0.865\n",
      "Epoch:  154 / 200  | Train loss:  0.042133455459964414  | Test Accuracy :  0.865\n",
      "Epoch:  155 / 200  | Train loss:  0.041773780447644626  | Test Accuracy :  0.865\n",
      "Epoch:  156 / 200  | Train loss:  0.04140627640700341  | Test Accuracy :  0.865\n",
      "Epoch:  157 / 200  | Train loss:  0.041059203754569874  | Test Accuracy :  0.865\n",
      "Epoch:  158 / 200  | Train loss:  0.04070627179931604  | Test Accuracy :  0.865\n",
      "Epoch:  159 / 200  | Train loss:  0.040357338511848986  | Test Accuracy :  0.865\n",
      "Epoch:  160 / 200  | Train loss:  0.040014243848125464  | Test Accuracy :  0.865\n",
      "Epoch:  161 / 200  | Train loss:  0.03967976270638538  | Test Accuracy :  0.865\n",
      "Epoch:  162 / 200  | Train loss:  0.03933863926637864  | Test Accuracy :  0.865\n",
      "Epoch:  163 / 200  | Train loss:  0.03900603478761184  | Test Accuracy :  0.865\n",
      "Epoch:  164 / 200  | Train loss:  0.038679143526663684  | Test Accuracy :  0.865\n",
      "Epoch:  165 / 200  | Train loss:  0.0383566719127186  | Test Accuracy :  0.865\n",
      "Epoch:  166 / 200  | Train loss:  0.03803674389327447  | Test Accuracy :  0.865\n",
      "Epoch:  167 / 200  | Train loss:  0.03772310298899935  | Test Accuracy :  0.865\n",
      "Epoch:  168 / 200  | Train loss:  0.03740666671536089  | Test Accuracy :  0.865\n",
      "Epoch:  169 / 200  | Train loss:  0.03710330670861581  | Test Accuracy :  0.865\n",
      "Epoch:  170 / 200  | Train loss:  0.036788939272153126  | Test Accuracy :  0.865\n",
      "Epoch:  171 / 200  | Train loss:  0.03649870365437468  | Test Accuracy :  0.865\n",
      "Epoch:  172 / 200  | Train loss:  0.036193554623489194  | Test Accuracy :  0.865\n",
      "Epoch:  173 / 200  | Train loss:  0.035900343728959  | Test Accuracy :  0.865\n",
      "Epoch:  174 / 200  | Train loss:  0.035608636415810205  | Test Accuracy :  0.865\n",
      "Epoch:  175 / 200  | Train loss:  0.035324686976143535  | Test Accuracy :  0.865\n",
      "Epoch:  176 / 200  | Train loss:  0.03503892949127887  | Test Accuracy :  0.865\n",
      "Epoch:  177 / 200  | Train loss:  0.034755757676809565  | Test Accuracy :  0.865\n",
      "Epoch:  178 / 200  | Train loss:  0.034478477846321534  | Test Accuracy :  0.865\n",
      "Epoch:  179 / 200  | Train loss:  0.03420356141780718  | Test Accuracy :  0.865\n",
      "Epoch:  180 / 200  | Train loss:  0.03393318551414287  | Test Accuracy :  0.865\n",
      "Epoch:  181 / 200  | Train loss:  0.03366652317473005  | Test Accuracy :  0.865\n",
      "Epoch:  182 / 200  | Train loss:  0.03339730982386516  | Test Accuracy :  0.865\n",
      "Epoch:  183 / 200  | Train loss:  0.03313654600664749  | Test Accuracy :  0.865\n",
      "Epoch:  184 / 200  | Train loss:  0.032875410541866566  | Test Accuracy :  0.865\n",
      "Epoch:  185 / 200  | Train loss:  0.032618007262857865  | Test Accuracy :  0.865\n",
      "Epoch:  186 / 200  | Train loss:  0.0323585887038467  | Test Accuracy :  0.865\n",
      "Epoch:  187 / 200  | Train loss:  0.032110833118714766  | Test Accuracy :  0.865\n",
      "Epoch:  188 / 200  | Train loss:  0.03186161786421382  | Test Accuracy :  0.865\n",
      "Epoch:  189 / 200  | Train loss:  0.03161279482354679  | Test Accuracy :  0.865\n",
      "Epoch:  190 / 200  | Train loss:  0.031370779704197146  | Test Accuracy :  0.865\n",
      "Epoch:  191 / 200  | Train loss:  0.03112835255309166  | Test Accuracy :  0.865\n",
      "Epoch:  192 / 200  | Train loss:  0.03088559108173184  | Test Accuracy :  0.865\n",
      "Epoch:  193 / 200  | Train loss:  0.030655048919377977  | Test Accuracy :  0.865\n",
      "Epoch:  194 / 200  | Train loss:  0.030419973245369592  | Test Accuracy :  0.865\n",
      "Epoch:  195 / 200  | Train loss:  0.030184162338535492  | Test Accuracy :  0.865\n",
      "Epoch:  196 / 200  | Train loss:  0.02995869632641832  | Test Accuracy :  0.865\n",
      "Epoch:  197 / 200  | Train loss:  0.029734221839824528  | Test Accuracy :  0.86\n",
      "Epoch:  198 / 200  | Train loss:  0.029502371560601415  | Test Accuracy :  0.86\n",
      "Epoch:  199 / 200  | Train loss:  0.029288340190600105  | Test Accuracy :  0.86\n",
      "Epoch:  200 / 200  | Train loss:  0.02906151035055399  | Test Accuracy :  0.86\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.86"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_path = './MNIST_Sub/train_data.txt'\n",
    "train_labels_path = './MNIST_Sub/train_labels.txt'\n",
    "test_data_path = './MNIST_Sub/test_data.txt'\n",
    "test_labels_path = './MNIST_Sub/test_labels.txt'\n",
    "\n",
    "\n",
    "#classifier\n",
    "classifier_layers_arch = [['Linear', (28*28, 256)], ['ReLU'], ['Linear', (256, 10)]]\n",
    "cls = Classifier(train_data_path, train_labels_path, test_data_path, test_labels_path, layers_arch = classifier_layers_arch, learning_rate = 0.01, batch_size = 32, max_epoch = 200)\n",
    "cls.Train()\n",
    "cls.Test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.2032403939693412, 0.7511231772382598, 0.6185338925466222, 0.543943894166101, 0.49357159517196036, 0.4558785678693509, 0.42597768662338614, 0.4013062958259187, 0.38044154007356196, 0.36238221124972964, 0.3465081594321483, 0.33223648822561963, 0.31940474664484386, 0.30764299094492353, 0.29689134406729223, 0.2869064422713618, 0.27768215700960464, 0.2690910782671333, 0.2610704085350784, 0.2535717369334196, 0.24647910314748533, 0.23985538521628005, 0.23356999211306192, 0.22766253667037806, 0.22203776867461683, 0.2166989615207931, 0.21160678609302813, 0.206712647716294, 0.2020944398686461, 0.1976371755296471, 0.19335331392246535, 0.18926015444260608, 0.18535995147885936, 0.18160852802523164, 0.17800734182520583, 0.17449074770255174, 0.17111515622199752, 0.16784548290615747, 0.16471118343395025, 0.16167208770933114, 0.15871900465224764, 0.15586864661220556, 0.15312561151748103, 0.15048150047698008, 0.1478794840042625, 0.14539869383216805, 0.14296021955558372, 0.14060795752417268, 0.1383195857164207, 0.13609927359657775, 0.1339148117361781, 0.13183093844605712, 0.12978304480323097, 0.12780954116975213, 0.12584998094179767, 0.12395997280244211, 0.12212551870377081, 0.12030223752011797, 0.1185378743519009, 0.1168350463166559, 0.11515103928935014, 0.11350506167575752, 0.11188525894084213, 0.11033795045133854, 0.10880482041812614, 0.10730447483425597, 0.10584973240134683, 0.10440893819568497, 0.10300581551608581, 0.10163691085701637, 0.10030361690490082, 0.09899905141362304, 0.0977099828495241, 0.09645518394143655, 0.09521532475057165, 0.09401227522792989, 0.0928341494469114, 0.09168528162517066, 0.09055333402183957, 0.08944112704147623, 0.08833990292078096, 0.0872700876181057, 0.08623484533321911, 0.08521570303308483, 0.0841905502730611, 0.08319872972005886, 0.0822367813953828, 0.08126186716496471, 0.08032914144668407, 0.07940992706996501, 0.07849038270281396, 0.07760231646943325, 0.07671344463701671, 0.07583829821173158, 0.0750025647548573, 0.07416655192569077, 0.07333593574051245, 0.0725225796564442, 0.07173905638958396, 0.0709519465253666, 0.0701916392411222, 0.06943615666813199, 0.06868859201105869, 0.06797219054955222, 0.06724718082930431, 0.06653471489236408, 0.065845585578282, 0.06516112469582522, 0.06448463190344399, 0.06381731565541303, 0.06316574366824095, 0.06252913441191339, 0.06189546260058111, 0.06126132153132238, 0.06064335208417447, 0.06004343069210016, 0.05944561267311772, 0.05885395518238472, 0.058281257019995696, 0.0577010451308618, 0.057136756276176634, 0.056586139467468946, 0.056037874718694966, 0.055490958481347795, 0.054960856833411856, 0.05443826397677451, 0.05391331962359846, 0.05340137165336332, 0.052893820472011306, 0.05239663390512464, 0.05190422876668707, 0.05141644715733765, 0.05093550523174944, 0.050463067809694055, 0.0499936310168162, 0.0495342702019441, 0.04908059915683017, 0.04862549968483204, 0.04818372299977876, 0.04775159269244393, 0.047313245222324926, 0.046882653283231886, 0.04645476540419059, 0.04604379096153592, 0.04562915254216444, 0.04522011721329485, 0.04481498616798417, 0.044427945578152106, 0.04402337702718911, 0.04363524982632535, 0.04326539875622512, 0.042879032612876615, 0.04250578423673591, 0.042133455459964414, 0.041773780447644626, 0.04140627640700341, 0.041059203754569874, 0.04070627179931604, 0.040357338511848986, 0.040014243848125464, 0.03967976270638538, 0.03933863926637864, 0.03900603478761184, 0.038679143526663684, 0.0383566719127186, 0.03803674389327447, 0.03772310298899935, 0.03740666671536089, 0.03710330670861581, 0.036788939272153126, 0.03649870365437468, 0.036193554623489194, 0.035900343728959, 0.035608636415810205, 0.035324686976143535, 0.03503892949127887, 0.034755757676809565, 0.034478477846321534, 0.03420356141780718, 0.03393318551414287, 0.03366652317473005, 0.03339730982386516, 0.03313654600664749, 0.032875410541866566, 0.032618007262857865, 0.0323585887038467, 0.032110833118714766, 0.03186161786421382, 0.03161279482354679, 0.031370779704197146, 0.03112835255309166, 0.03088559108173184, 0.030655048919377977, 0.030419973245369592, 0.030184162338535492, 0.02995869632641832, 0.029734221839824528, 0.029502371560601415, 0.029288340190600105, 0.02906151035055399]\n",
      "[0.435, 0.58, 0.69, 0.725, 0.75, 0.785, 0.8, 0.805, 0.81, 0.805, 0.81, 0.82, 0.82, 0.82, 0.82, 0.82, 0.825, 0.835, 0.845, 0.85, 0.855, 0.855, 0.855, 0.855, 0.855, 0.855, 0.86, 0.86, 0.86, 0.86, 0.86, 0.855, 0.855, 0.855, 0.855, 0.855, 0.855, 0.855, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.855, 0.86, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.87, 0.87, 0.87, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.87, 0.87, 0.87, 0.865, 0.87, 0.865, 0.87, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.87, 0.87, 0.87, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.87, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.86, 0.86, 0.86, 0.86]\n"
     ]
    }
   ],
   "source": [
    "print(cls.ploss)\n",
    "print(cls.pacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1,1)\n",
    "\n",
    "axs.plot(cls.pepochs, cls.ploss)\n",
    "axs.set_title(\"Loss Convergence Rate\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig2, axs2 = plt.subplots(1,1)\n",
    "\n",
    "axs2.plot(cls.pepochs, cls.pacc)\n",
    "axs2.set_title(\"Testing Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
