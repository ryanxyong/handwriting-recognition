{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entree Task: Implementing Your Own Neural Networks from Scratch\n",
    "## By Vaani Gupta and Ryan Yong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Linear Layer \n",
    "Implement the forward and backward functions for a linear layer. Please read the requirement details for Task 1 in the code comment and in the pdf document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    def __init__(self, _m, _n):\n",
    "        '''\n",
    "        :param _m: _m is the input X hidden size\n",
    "        :param _n: _n is the output Y hidden size\n",
    "        '''\n",
    "        # \"Kaiming initialization\" is important for neural network to converge. The NN will not converge without it!\n",
    "        self.W = (np.random.uniform(low=-10000.0, high=10000.0, size = (_m, _n)))/10000.0*np.sqrt(6.0/ _m)\n",
    "        self.stored_X = None\n",
    "        self.W_grad = None #record the gradient of the weight\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        :param X: shape(X)[0] is batch size and shape(X)[1] is the #features\n",
    "         (1) Store the input X in stored_data for Backward.\n",
    "         (2) :return: X * weights\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        # Stores the input X in stored_data for Backward\n",
    "        self.stored_X = X\n",
    "        # Calculates and returns the forward operation of X * weights\n",
    "        return X @ self.W\n",
    "        \n",
    "        ##########  Code end   ##########\n",
    "    \n",
    "    def backward(self, Y_grad):\n",
    "        '''\n",
    "        /* shape(output_grad)[0] is batch size and shape(output_grad)[1] is the # output features (shape(weight)[1])\n",
    "         * 1) Calculate the gradient of the output (the result of the Forward method) w.r.t. the **W** and store the product of the gradient and Y_grad in W_grad\n",
    "         * 2) Calculate the gradient of the output (the result of the Forward method) w.r.t. the **X** and return the product of the gradient and Y_grad\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        # Calculates W gradient of output from Forward method and stores product of gradient and Y_grad in W_grad\n",
    "        self.W_grad = self.stored_X.T @ Y_grad\n",
    "        # Calculates X gradient of output from Forward method and returns product of gradient and Y_grad\n",
    "        return Y_grad @ self.W.T\n",
    "        ##########  Code end   ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 1: Linear Layer\n",
    "Check your linear forward and backward function implementations with numerical derivatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your gradient:  [[-0.80899972 -0.21857053 -0.25933788]]\n",
      "Numerical gradient: [[-0.80899972 -0.21857053 -0.25933788]]\n",
      "Error:  9.110101562015416e-11\n",
      "Correct backward. Congratulations!\n"
     ]
    }
   ],
   "source": [
    "#gradient check\n",
    "import copy\n",
    "#Random test\n",
    "n = 3\n",
    "m = 6\n",
    "Y_grad = np.random.rand(1, m)\n",
    "test_vector = np.random.rand(1, n)\n",
    "DELTA = 1e-6\n",
    "test_layer = LinearLayer(n, m)\n",
    "\n",
    "test_layer_1 = copy.deepcopy(test_layer)\n",
    "test_layer_2 = copy.deepcopy(test_layer)\n",
    "\n",
    "test_layer.forward(test_vector)\n",
    "Your_backward = test_layer.backward(Y_grad)\n",
    "\n",
    "cal_gradient = np.zeros((np.shape(test_vector)[0], np.shape(test_vector)[1]))\n",
    "for t_p in range(np.shape(test_vector)[0]):\n",
    "    for i in range(np.shape(test_vector)[1]):\n",
    "        test_vector_1 = copy.deepcopy(test_vector)\n",
    "        test_vector_2 = copy.deepcopy(test_vector)\n",
    "        test_vector_1[t_p][i] = test_vector_1[t_p][i] + DELTA\n",
    "        test_vector_2[t_p][i] = test_vector_2[t_p][i] - DELTA\n",
    "\n",
    "        cal_gradient[t_p][i] = np.sum(\n",
    "            (np.dot(Y_grad, np.transpose(test_layer_1.forward(test_vector_1) - test_layer_2.forward(test_vector_2))/(2*DELTA))))\n",
    "\n",
    "\n",
    "print('Your gradient: ',Your_backward)\n",
    "print('Numerical gradient:',cal_gradient)\n",
    "print('Error: ',abs(np.sum(Your_backward - cal_gradient)))\n",
    "if abs(np.sum(Your_backward - cal_gradient)) < 1e-4:\n",
    "    print('Correct backward. Congratulations!')\n",
    "else:\n",
    "    print('Wrong backawrd. Please check your implementation again.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Non-Linear Activation\n",
    "Implement the forward and backward functions for a nonlinear layer. Please read the requirement details for Task 2 in the code comment and in the pdf document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    #sigmoid layer\n",
    "    def __init__(self):\n",
    "        self.stored_X = None # Here we should store the input matrix X for Backward\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        /*\n",
    "         *  The input X matrix has the dimension [#samples, #features].\n",
    "         *  The output Y matrix has the same dimension as the input X.\n",
    "         *  You need to perform ReLU on each element of the input matrix to calculate the output matrix.\n",
    "         *  TODO: 1) Create an output matrix by going through each element in input and calculate relu=max(0,x) and\n",
    "         *  TODO: 2) Store the input X in self.stored_X for Backward.\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        self.stored_X = X\n",
    "        Y = np.zeros(X.shape)\n",
    "        \n",
    "        # Loops through each element of the input X matrix and stores the max of the current element in X and \n",
    "        # 0 in the output Y matrix\n",
    "        for i in range(X.shape[0]):\n",
    "            for j in range(X.shape[1]):\n",
    "                Y[i][j] = max(0, X[i][j])\n",
    "        \n",
    "        return Y\n",
    "                \n",
    "        ##########  Code end   ##########\n",
    "\n",
    "    def backward(self, Y_grad):\n",
    "        '''\n",
    "         /*  grad_relu(x)=1 if relu(x)=x\n",
    "         *  grad_relu(x)=0 if relu(x)=0\n",
    "         *\n",
    "         *  The input matrix has the name \"output_grad.\" The name is confusing (it is actually the input of the function). But the name follows the convension in PyTorch.\n",
    "         *  The output matrix has the same dimension as input.\n",
    "         *  The output matrix is calculated as grad_relu(stored_X)*Y_grad.\n",
    "         *  TODO: returns the output matrix calculated above\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        grad = np.zeros(Y_grad.shape)\n",
    "        \n",
    "        # Loops through all elements of the input X matrix and stores the value of the element in Y_grad only\n",
    "        # if its value is greater than 0 in the output grad matrix\n",
    "        for i in range(self.stored_X.shape[0]):\n",
    "            for j in range(self.stored_X.shape[1]):\n",
    "                if (self.stored_X[i][j] > 0):\n",
    "                    grad[i][j] = Y_grad[i][j]\n",
    "                else:\n",
    "                    grad[i][j] = 0  \n",
    "        \n",
    "        return grad\n",
    "\n",
    "        ##########  Code end   ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 2: ReLU \n",
    "Check your ReLU forward and backward functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your gradient:  [[0.27021522 0.86796804 0.12691432]]\n",
      "Numerical gradient: [[0.27021522 0.86796804 0.12691432]]\n",
      "Error:  1.785624426098309e-11\n",
      "Correct backward. Congratulations!\n"
     ]
    }
   ],
   "source": [
    "#gradient check\n",
    "import copy\n",
    "#Random test\n",
    "n = 3\n",
    "Y_grad = np.random.rand(1, n)\n",
    "test_vector = np.random.rand(1, n)\n",
    "DELTA = 1e-6\n",
    "test_layer = ReLU()\n",
    "\n",
    "test_layer_1 = copy.deepcopy(test_layer)\n",
    "test_layer_2 = copy.deepcopy(test_layer)\n",
    "\n",
    "test_layer.forward(test_vector)\n",
    "Your_backward = test_layer.backward(Y_grad)\n",
    "\n",
    "cal_gradient = np.zeros((np.shape(test_vector)[0], np.shape(test_vector)[1]))\n",
    "for t_p in range(np.shape(test_vector)[0]):\n",
    "    for i in range(np.shape(test_vector)[1]):\n",
    "        test_vector_1 = copy.deepcopy(test_vector)\n",
    "        test_vector_2 = copy.deepcopy(test_vector)\n",
    "        test_vector_1[t_p][i] = test_vector_1[t_p][i] + DELTA\n",
    "        test_vector_2[t_p][i] = test_vector_2[t_p][i] - DELTA\n",
    "\n",
    "        cal_gradient[t_p][i] = np.sum(\n",
    "            (np.dot(Y_grad, np.transpose(test_layer_1.forward(test_vector_1) - test_layer_2.forward(test_vector_2))/(2*DELTA))))\n",
    "\n",
    "\n",
    "print('Your gradient: ',Your_backward)\n",
    "print('Numerical gradient:',cal_gradient)\n",
    "print('Error: ',abs(np.sum(Your_backward - cal_gradient)))\n",
    "if abs(np.sum(Your_backward - cal_gradient)) < 1e-4:\n",
    "    print('Correct backward. Congratulations!')\n",
    "else:\n",
    "    print('Wrong backawrd. Please check your implementation again.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Loss Function\n",
    "Implement the MSE loss function and its backward derivative. Please read the requirement details for Task 3 in the code comment and in the pdf document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    # cross entropy loss\n",
    "    # return the mse loss mean(y_j-y_pred_i)^2\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stored_diff = None\n",
    "    def forward(self, prediction, groundtruth):\n",
    "        '''\n",
    "        /*  TODO: 1) Calculate stored_data=pred-truth\n",
    "         *  TODO: 2) Calculate the MSE loss as the squared sum of all the elements in the stored_data divided by the number of elements, i.e., MSE(pred, truth) = ||pred-truth||^2 / N, with N as the total number of elements in the matrix\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        # Stores the difference between the prediction and ground truth \n",
    "        self.stored_diff = prediction - groundtruth\n",
    "        \n",
    "        # Finds sum of squares of difference between prediction and ground truth\n",
    "        sumSquare = np.sum(np.square(prediction - groundtruth))\n",
    "        \n",
    "        # Divide sum by number of samples \n",
    "        return sumSquare/prediction.shape[0]\n",
    "        ##########  Code end   ##########\n",
    "\n",
    "    # return the gradient of the input data\n",
    "    def backward(self):\n",
    "        '''\n",
    "        /* TODO: return the gradient matrix of the MSE loss\n",
    "         * The output matrix has the same dimension as the stored_data (make sure you have stored the (pred-truth) in stored_data in your forward function!)\n",
    "         * Each element (i,j) of the output matrix is calculated as grad(i,j)=2(pred(i,j)-truth(i,j))/N\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        # Returns 2 times the difference between the prediction and ground truth divided by number of samples\n",
    "        return (2/self.stored_diff.shape[0]) * self.stored_diff\n",
    "        ##########  Code end   ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Network Architecture\n",
    "Implement your own neural network architecture. Please read the requirement for Task 4 in the pdf document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, layers_arch):\n",
    "        '''\n",
    "        /*  TODO: 1) Initialize the array for input layers with the proper feature sizes specified in the input vector.\n",
    "         * For the linear layer, in each pair (in_size, out_size), the in_size is the feature size of the previous layer and the out_size is the feature size of the output (that goes to the next layer)\n",
    "         * In the linear layer, the weight should have the shape (in_size, out_size).\n",
    "         \n",
    "         *  For example, if layers_arch = [['Linear', (256, 128)], ['ReLU'], ['Linear', (128, 64)], ['ReLU'], ['Linear', (64, 32)]],\n",
    "       * \t\t\t\t\t\t\t then there are three linear layers whose weights are with shapes (256, 128), (128, 64), (64, 32),\n",
    "       * \t\t\t\t\t\t\t and there are two non-linear layers.\n",
    "         *  Attention: * The output feature size of the linear layer i should always equal to the input feature size of the linear layer i+1.\n",
    "       */\n",
    "        '''\n",
    "       \n",
    "        ########## Code start  ##########\n",
    "        self.layers = []\n",
    "\n",
    "        # Loops through all layers in the network architecture and stores each layer as a linear \n",
    "        # or ReLU layer in the Network's layers array\n",
    "        for layer in layers_arch:\n",
    "            if layer[0] == 'Linear':\n",
    "                self.layers.append(LinearLayer(layer[1][0], layer[1][1]))\n",
    "            else:\n",
    "                self.layers.append(ReLU())\n",
    "            \n",
    "        ##########  Code end   ##########\n",
    "        \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        /*\n",
    "         * TODO: propagate the input data for the first linear layer throught all the layers in the network and return the output of the last linear layer.\n",
    "         * For implementation, you need to write a for-loop to propagate the input from the first layer to the last layer (before the loss function) by going through the forward functions of all the layers.\n",
    "         * For example, for a network with k linear layers and k-1 activation layers, the data flow is:\n",
    "         * linear[0] -> activation[0] -> linear[1] ->activation[1] -> ... -> linear[k-2] -> activation[k-2] -> linear[k-1]\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        \n",
    "        curr_x = X\n",
    "        \n",
    "        # Loops through all layers and performs the forward method on them\n",
    "        for i in range(len(self.layers)):\n",
    "            curr_x = self.layers[i].forward(curr_x)\n",
    "        \n",
    "        return curr_x\n",
    "\n",
    "        ##########  Code end   ##########\n",
    "\n",
    "    def backward(self, Y_grad):\n",
    "        '''\n",
    "        /* Propagate the gradient from the last layer to the first layer by going through the backward functions of all the layers.\n",
    "         * TODO: propagate the gradient of the output (we got from the Forward method) back throught the network and return the gradient of the first layer.\n",
    "\n",
    "         * Notice: We should use the chain rule for the backward.\n",
    "         * Notice: The order is opposite to the forward.\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        curr_y_grad = Y_grad\n",
    "        \n",
    "        # Loops through all layers and performs the backward method on them\n",
    "        for i in range(len(self.layers)-1, -1, -1):\n",
    "            curr_y_grad = self.layers[i].backward(curr_y_grad)\n",
    "        \n",
    "        return curr_y_grad\n",
    "            \n",
    "        ##########  Code end   ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 3: Regression Network\n",
    "Check your network implementation with a simple regression task. Here we also provide you a sample implementation for the gradient descent algorithm, which you will find useful for your own Classifier implementation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor:\n",
    "    #Classifier\n",
    "    def __init__(self, layers_arch, data_function, learning_rate = 1e-3, batch_size = 32, max_epoch = 200):\n",
    "\n",
    "        input_feature_size = 2\n",
    "        output_feature_size = 2\n",
    "\n",
    "        self.train_data = []\n",
    "        self.train_label = []\n",
    "        self.test_data = []\n",
    "        self.test_label = []\n",
    "\n",
    "        self.data_function = data_function\n",
    "        \n",
    "        self.layers_arch = layers_arch\n",
    "        self.net = Network(layers_arch)\n",
    "        self.loss_function = MSELoss()\n",
    "\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def dataloader(self):\n",
    "        \n",
    "        '''\n",
    "        We randomly generate the mapping: (x)->(x^3+x^2 + 1)\n",
    "        '''\n",
    "        self.train_data = np.zeros((1000,1))\n",
    "        self.train_label = np.zeros((1000, 1))\n",
    "\n",
    "        for i in range(1000):\n",
    "            self.train_data[i][0] = np.random.uniform(low=0.0, high=10000.0)/10000.0\n",
    "            self.train_label[i][0] = self.data_function(self.train_data[i][0])\n",
    "\n",
    "        self.test_data = np.zeros((200, 1))\n",
    "        self.test_label = np.zeros((200, 1))\n",
    "\n",
    "        for i in range(200):\n",
    "            self.test_data[i][0] = np.random.uniform(low=-0.0, high=10000.0) / 10000.0\n",
    "            self.test_label[i][0] = self.data_function(self.test_data[i][0])\n",
    "\n",
    "\n",
    "\n",
    "    def Train_One_Epoch(self):\n",
    "        '''\n",
    "        Here we train the network using gradient descent\n",
    "        '''\n",
    "        loss = 0\n",
    "        n_loop = int(math.ceil(len(self.train_data)/self.batch_size))\n",
    "    \n",
    "\n",
    "        for i in range(n_loop):\n",
    "            batch_data = self.train_data[i * self.batch_size : (i+1)*self.batch_size]\n",
    "            batch_label = self.train_label[i * self.batch_size : (i+1)*self.batch_size]\n",
    "            \n",
    "            '''\n",
    "            /*  Forward the data to the network.\n",
    "             *  Forward the result to the loss function.\n",
    "             *  Backward.\n",
    "             *  Update the weights with weight gradients.\n",
    "             *  Do not forget the learning rate!\n",
    "             */\n",
    "            '''\n",
    "            \n",
    "            ########## Sample code  ##########\n",
    "            prediction = self.net.forward(batch_data)\n",
    "            loss += self.loss_function.forward(prediction, batch_label)\n",
    "\n",
    "            pred_grad = self.loss_function.backward()\n",
    "            self.net.backward(pred_grad)\n",
    "            \n",
    "            \n",
    "            for i in range(len(self.layers_arch)):\n",
    "                if self.layers_arch[i][0] == 'Linear':\n",
    "                    self.net.layers[i].W -= self.net.layers[i].W_grad * self.learning_rate\n",
    "            ##########  Sample code ##########\n",
    "            \n",
    "        return loss/n_loop\n",
    "\n",
    "    def Test(self):\n",
    "        prediction = self.net.forward(self.test_data)\n",
    "        loss = self.loss_function.forward(prediction, self.test_label)\n",
    "        return loss\n",
    "\n",
    "    def Train(self):\n",
    "        self.dataloader()\n",
    "        for i in range(self.max_epoch):\n",
    "            train_loss = self.Train_One_Epoch()\n",
    "            test_loss = self.Test()\n",
    "            print(\"Epoch: \", str(i+1), \"/\", str(self.max_epoch), \" | Train loss: \", train_loss, \" | Test loss : \", test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 / 200  | Train loss:  1.523359006708903  | Test loss :  1.2215882192986363\n",
      "Epoch:  2 / 200  | Train loss:  1.2015056555672097  | Test loss :  0.9684808481817379\n",
      "Epoch:  3 / 200  | Train loss:  0.9520166529069569  | Test loss :  0.772296936829498\n",
      "Epoch:  4 / 200  | Train loss:  0.7589163241839182  | Test loss :  0.6204913878534434\n",
      "Epoch:  5 / 200  | Train loss:  0.6097503702825209  | Test loss :  0.5032582754276054\n",
      "Epoch:  6 / 200  | Train loss:  0.4947730870569282  | Test loss :  0.4129151477065187\n",
      "Epoch:  7 / 200  | Train loss:  0.406348960058301  | Test loss :  0.343442021278679\n",
      "Epoch:  8 / 200  | Train loss:  0.33849909309312287  | Test loss :  0.290127457942416\n",
      "Epoch:  9 / 200  | Train loss:  0.28654993338277795  | Test loss :  0.2492920566881641\n",
      "Epoch:  10 / 200  | Train loss:  0.2468573112115636  | Test loss :  0.2180700879495136\n",
      "Epoch:  11 / 200  | Train loss:  0.2165878748052055  | Test loss :  0.19423610898568533\n",
      "Epoch:  12 / 200  | Train loss:  0.19354538583049755  | Test loss :  0.17606708959090675\n",
      "Epoch:  13 / 200  | Train loss:  0.17603264978427038  | Test loss :  0.162232903309841\n",
      "Epoch:  14 / 200  | Train loss:  0.1627419946841477  | Test loss :  0.15170959445323629\n",
      "Epoch:  15 / 200  | Train loss:  0.15266868571289943  | Test loss :  0.14371094280116078\n",
      "Epoch:  16 / 200  | Train loss:  0.14504274878879336  | Test loss :  0.1376346929843145\n",
      "Epoch:  17 / 200  | Train loss:  0.1392755205432377  | Test loss :  0.13302048843663092\n",
      "Epoch:  18 / 200  | Train loss:  0.13491792460840296  | Test loss :  0.12951710078790146\n",
      "Epoch:  19 / 200  | Train loss:  0.13162803691703073  | Test loss :  0.12685700242649797\n",
      "Epoch:  20 / 200  | Train loss:  0.1291459702317584  | Test loss :  0.1248367095852079\n",
      "Epoch:  21 / 200  | Train loss:  0.12727449603778332  | Test loss :  0.12330163746019697\n",
      "Epoch:  22 / 200  | Train loss:  0.1258641419384723  | Test loss :  0.12213446697303425\n",
      "Epoch:  23 / 200  | Train loss:  0.12480176456996832  | Test loss :  0.1212462330233311\n",
      "Epoch:  24 / 200  | Train loss:  0.12400181048950812  | Test loss :  0.12056951383230496\n",
      "Epoch:  25 / 200  | Train loss:  0.12339964833780563  | Test loss :  0.12005323689008883\n",
      "Epoch:  26 / 200  | Train loss:  0.1229464918536672  | Test loss :  0.1196587249919223\n",
      "Epoch:  27 / 200  | Train loss:  0.12260554121331152  | Test loss :  0.11935669102065212\n",
      "Epoch:  28 / 200  | Train loss:  0.12234905501692334  | Test loss :  0.11912495689780378\n",
      "Epoch:  29 / 200  | Train loss:  0.1221561315751205  | Test loss :  0.11894672417038896\n",
      "Epoch:  30 / 200  | Train loss:  0.12201102972800792  | Test loss :  0.11880926407295789\n",
      "Epoch:  31 / 200  | Train loss:  0.1219018993543636  | Test loss :  0.11870292608839311\n",
      "Epoch:  32 / 200  | Train loss:  0.12181982250717953  | Test loss :  0.11862038802810858\n",
      "Epoch:  33 / 200  | Train loss:  0.12175808975600802  | Test loss :  0.11855608905922554\n",
      "Epoch:  34 / 200  | Train loss:  0.1217116544244215  | Test loss :  0.11850580118558227\n",
      "Epoch:  35 / 200  | Train loss:  0.12167672124177893  | Test loss :  0.11846630543239488\n",
      "Epoch:  36 / 200  | Train loss:  0.12165043646813173  | Test loss :  0.11843514716440932\n",
      "Epoch:  37 / 200  | Train loss:  0.1216306545665483  | Test loss :  0.11841045118464226\n",
      "Epoch:  38 / 200  | Train loss:  0.12161576258228807  | Test loss :  0.1183907819789156\n",
      "Epoch:  39 / 200  | Train loss:  0.12160454800097746  | Test loss :  0.11837503804706881\n",
      "Epoch:  40 / 200  | Train loss:  0.12159609934993604  | Test loss :  0.1183623719685368\n",
      "Epoch:  41 / 200  | Train loss:  0.12158973144735916  | Test loss :  0.1183521298970802\n",
      "Epoch:  42 / 200  | Train loss:  0.12158492919882034  | Test loss :  0.11834380572638466\n",
      "Epoch:  43 / 200  | Train loss:  0.1215813053461737  | Test loss :  0.1183370063364384\n",
      "Epoch:  44 / 200  | Train loss:  0.12157856870951506  | Test loss :  0.11833142521229942\n",
      "Epoch:  45 / 200  | Train loss:  0.12157650031880009  | Test loss :  0.11832682239205171\n",
      "Epoch:  46 / 200  | Train loss:  0.12157493547652519  | Test loss :  0.11832300920243025\n",
      "Epoch:  47 / 200  | Train loss:  0.12157375027839777  | Test loss :  0.11831983661887194\n",
      "Epoch:  48 / 200  | Train loss:  0.12157285148435518  | Test loss :  0.11831718637194207\n",
      "Epoch:  49 / 200  | Train loss:  0.1215721689072434  | Test loss :  0.11831496413709054\n",
      "Epoch:  50 / 200  | Train loss:  0.12157164969327364  | Test loss :  0.11831309430678835\n",
      "Epoch:  51 / 200  | Train loss:  0.12157125402388923  | Test loss :  0.11831151596632122\n",
      "Epoch:  52 / 200  | Train loss:  0.12157095188558961  | Test loss :  0.11831017978669901\n",
      "Epoch:  53 / 200  | Train loss:  0.12157072064213861  | Test loss :  0.11830904561768628\n",
      "Epoch:  54 / 200  | Train loss:  0.1215705432096294  | Test loss :  0.11830808061644842\n",
      "Epoch:  55 / 200  | Train loss:  0.12157040668450639  | Test loss :  0.11830725778694694\n",
      "Epoch:  56 / 200  | Train loss:  0.12157030131193425  | Test loss :  0.11830655483516807\n",
      "Epoch:  57 / 200  | Train loss:  0.1215702197099166  | Test loss :  0.11830595326791961\n",
      "Epoch:  58 / 200  | Train loss:  0.12157015628561404  | Test loss :  0.11830543768007373\n",
      "Epoch:  59 / 200  | Train loss:  0.12157010679611825  | Test loss :  0.11830499518812218\n",
      "Epoch:  60 / 200  | Train loss:  0.12157006801781546  | Test loss :  0.11830461497776497\n",
      "Epoch:  61 / 200  | Train loss:  0.12157003749739154  | Test loss :  0.11830428794073697\n",
      "Epoch:  62 / 200  | Train loss:  0.12157001336423129  | Test loss :  0.1183040063817732\n",
      "Epoch:  63 / 200  | Train loss:  0.12156999418899743  | Test loss :  0.11830376378095313\n",
      "Epoch:  64 / 200  | Train loss:  0.12156997887695535  | Test loss :  0.11830355459997972\n",
      "Epoch:  65 / 200  | Train loss:  0.12156996658744879  | Test loss :  0.11830337412348645\n",
      "Epoch:  66 / 200  | Train loss:  0.12156995667306672  | Test loss :  0.11830321832841177\n",
      "Epoch:  67 / 200  | Train loss:  0.12156994863364198  | Test loss :  0.11830308377598055\n",
      "Epoch:  68 / 200  | Train loss:  0.12156994208142981  | Test loss :  0.11830296752198574\n",
      "Epoch:  69 / 200  | Train loss:  0.12156993671471583  | Test loss :  0.11830286704196237\n",
      "Epoch:  70 / 200  | Train loss:  0.12156993229778677  | Test loss :  0.1183027801685383\n",
      "Epoch:  71 / 200  | Train loss:  0.121569928645705  | Test loss :  0.11830270503879278\n",
      "Epoch:  72 / 200  | Train loss:  0.12156992561271604  | Test loss :  0.11830264004987692\n",
      "Epoch:  73 / 200  | Train loss:  0.12156992308340342  | Test loss :  0.11830258382148581\n",
      "Epoch:  74 / 200  | Train loss:  0.12156992096592595  | Test loss :  0.11830253516403776\n",
      "Epoch:  75 / 200  | Train loss:  0.12156991918683427  | Test loss :  0.11830249305162424\n",
      "Epoch:  76 / 200  | Train loss:  0.12156991768708694  | Test loss :  0.11830245659896488\n",
      "Epoch:  77 / 200  | Train loss:  0.12156991641897966  | Test loss :  0.1183024250417342\n",
      "Epoch:  78 / 200  | Train loss:  0.12156991534377042  | Test loss :  0.1183023977197371\n",
      "Epoch:  79 / 200  | Train loss:  0.12156991442983661  | Test loss :  0.11830237406249731\n",
      "Epoch:  80 / 200  | Train loss:  0.1215699136512393  | Test loss :  0.11830235357689542\n",
      "Epoch:  81 / 200  | Train loss:  0.12156991298659982  | Test loss :  0.11830233583655189\n",
      "Epoch:  82 / 200  | Train loss:  0.1215699124182179  | Test loss :  0.11830232047269834\n",
      "Epoch:  83 / 200  | Train loss:  0.12156991193137501  | Test loss :  0.11830230716632141\n",
      "Epoch:  84 / 200  | Train loss:  0.12156991151378219  | Test loss :  0.1183022956413951\n",
      "Epoch:  85 / 200  | Train loss:  0.12156991115513995  | Test loss :  0.11830228565904793\n",
      "Epoch:  86 / 200  | Train loss:  0.12156991084678592  | Test loss :  0.11830227701253065\n",
      "Epoch:  87 / 200  | Train loss:  0.12156991058141103  | Test loss :  0.11830226952287387\n",
      "Epoch:  88 / 200  | Train loss:  0.12156991035282969  | Test loss :  0.11830226303513804\n",
      "Epoch:  89 / 200  | Train loss:  0.1215699101557932  | Test loss :  0.11830225741517403\n",
      "Epoch:  90 / 200  | Train loss:  0.12156990998583703  | Test loss :  0.11830225254682368\n",
      "Epoch:  91 / 200  | Train loss:  0.12156990983915546  | Test loss :  0.11830224832949954\n",
      "Epoch:  92 / 200  | Train loss:  0.12156990971249776  | Test loss :  0.11830224467609231\n",
      "Epoch:  93 / 200  | Train loss:  0.12156990960308278  | Test loss :  0.11830224151116063\n",
      "Epoch:  94 / 200  | Train loss:  0.12156990950852721  | Test loss :  0.11830223876936508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  95 / 200  | Train loss:  0.12156990942678593  | Test loss :  0.11830223639411319\n",
      "Epoch:  96 / 200  | Train loss:  0.12156990935610197  | Test loss :  0.11830223433638676\n",
      "Epoch:  97 / 200  | Train loss:  0.12156990929496429  | Test loss :  0.11830223255372677\n",
      "Epoch:  98 / 200  | Train loss:  0.12156990924207199  | Test loss :  0.11830223100935475\n",
      "Epoch:  99 / 200  | Train loss:  0.12156990919630446  | Test loss :  0.118302229671412\n",
      "Epoch:  100 / 200  | Train loss:  0.12156990915669537  | Test loss :  0.11830222851230097\n",
      "Epoch:  101 / 200  | Train loss:  0.12156990912241106  | Test loss :  0.11830222750811487\n",
      "Epoch:  102 / 200  | Train loss:  0.12156990909273208  | Test loss :  0.11830222663814378\n",
      "Epoch:  103 / 200  | Train loss:  0.12156990906703696  | Test loss :  0.11830222588444694\n",
      "Epoch:  104 / 200  | Train loss:  0.1215699090447888  | Test loss :  0.11830222523148227\n",
      "Epoch:  105 / 200  | Train loss:  0.12156990902552366  | Test loss :  0.11830222466578565\n",
      "Epoch:  106 / 200  | Train loss:  0.12156990900884032  | Test loss :  0.11830222417569282\n",
      "Epoch:  107 / 200  | Train loss:  0.12156990899439184  | Test loss :  0.11830222375109875\n",
      "Epoch:  108 / 200  | Train loss:  0.12156990898187817  | Test loss :  0.1183022233832493\n",
      "Epoch:  109 / 200  | Train loss:  0.12156990897103967  | Test loss :  0.11830222306456044\n",
      "Epoch:  110 / 200  | Train loss:  0.12156990896165171  | Test loss :  0.11830222278846186\n",
      "Epoch:  111 / 200  | Train loss:  0.12156990895351978  | Test loss :  0.11830222254926141\n",
      "Epoch:  112 / 200  | Train loss:  0.12156990894647556  | Test loss :  0.1183022223420278\n",
      "Epoch:  113 / 200  | Train loss:  0.12156990894037344  | Test loss :  0.11830222216248874\n",
      "Epoch:  114 / 200  | Train loss:  0.1215699089350872  | Test loss :  0.11830222200694304\n",
      "Epoch:  115 / 200  | Train loss:  0.12156990893050765  | Test loss :  0.1183022218721842\n",
      "Epoch:  116 / 200  | Train loss:  0.12156990892654015  | Test loss :  0.1183022217554341\n",
      "Epoch:  117 / 200  | Train loss:  0.12156990892310289  | Test loss :  0.11830222165428605\n",
      "Epoch:  118 / 200  | Train loss:  0.12156990892012483  | Test loss :  0.11830222156665496\n",
      "Epoch:  119 / 200  | Train loss:  0.12156990891754466  | Test loss :  0.11830222149073442\n",
      "Epoch:  120 / 200  | Train loss:  0.12156990891530904  | Test loss :  0.11830222142495944\n",
      "Epoch:  121 / 200  | Train loss:  0.12156990891337195  | Test loss :  0.11830222136797415\n",
      "Epoch:  122 / 200  | Train loss:  0.12156990891169348  | Test loss :  0.11830222131860388\n",
      "Epoch:  123 / 200  | Train loss:  0.12156990891023901  | Test loss :  0.11830222127583098\n",
      "Epoch:  124 / 200  | Train loss:  0.12156990890897865  | Test loss :  0.1183022212387738\n",
      "Epoch:  125 / 200  | Train loss:  0.12156990890788638  | Test loss :  0.1183022212066685\n",
      "Epoch:  126 / 200  | Train loss:  0.12156990890693974  | Test loss :  0.11830222117885324\n",
      "Epoch:  127 / 200  | Train loss:  0.12156990890611935  | Test loss :  0.11830222115475479\n",
      "Epoch:  128 / 200  | Train loss:  0.12156990890540827  | Test loss :  0.11830222113387644\n",
      "Epoch:  129 / 200  | Train loss:  0.12156990890479182  | Test loss :  0.11830222111578777\n",
      "Epoch:  130 / 200  | Train loss:  0.12156990890425744  | Test loss :  0.11830222110011608\n",
      "Epoch:  131 / 200  | Train loss:  0.12156990890379415  | Test loss :  0.11830222108653832\n",
      "Epoch:  132 / 200  | Train loss:  0.12156990890339246  | Test loss :  0.11830222107477466\n",
      "Epoch:  133 / 200  | Train loss:  0.12156990890304405  | Test loss :  0.11830222106458266\n",
      "Epoch:  134 / 200  | Train loss:  0.12156990890274191  | Test loss :  0.11830222105575232\n",
      "Epoch:  135 / 200  | Train loss:  0.12156990890247979  | Test loss :  0.11830222104810165\n",
      "Epoch:  136 / 200  | Train loss:  0.12156990890225239  | Test loss :  0.11830222104147299\n",
      "Epoch:  137 / 200  | Train loss:  0.12156990890205499  | Test loss :  0.11830222103572975\n",
      "Epoch:  138 / 200  | Train loss:  0.12156990890188363  | Test loss :  0.11830222103075366\n",
      "Epoch:  139 / 200  | Train loss:  0.1215699089017349  | Test loss :  0.11830222102644215\n",
      "Epoch:  140 / 200  | Train loss:  0.12156990890160564  | Test loss :  0.1183022210227064\n",
      "Epoch:  141 / 200  | Train loss:  0.12156990890149336  | Test loss :  0.11830222101946955\n",
      "Epoch:  142 / 200  | Train loss:  0.12156990890139571  | Test loss :  0.11830222101666482\n",
      "Epoch:  143 / 200  | Train loss:  0.12156990890131075  | Test loss :  0.11830222101423452\n",
      "Epoch:  144 / 200  | Train loss:  0.12156990890123683  | Test loss :  0.11830222101212862\n",
      "Epoch:  145 / 200  | Train loss:  0.12156990890117247  | Test loss :  0.11830222101030373\n",
      "Epoch:  146 / 200  | Train loss:  0.1215699089011163  | Test loss :  0.1183022210087223\n",
      "Epoch:  147 / 200  | Train loss:  0.12156990890106739  | Test loss :  0.11830222100735184\n",
      "Epoch:  148 / 200  | Train loss:  0.12156990890102458  | Test loss :  0.11830222100616412\n",
      "Epoch:  149 / 200  | Train loss:  0.12156990890098722  | Test loss :  0.11830222100513474\n",
      "Epoch:  150 / 200  | Train loss:  0.12156990890095454  | Test loss :  0.11830222100424251\n",
      "Epoch:  151 / 200  | Train loss:  0.12156990890092575  | Test loss :  0.11830222100346913\n",
      "Epoch:  152 / 200  | Train loss:  0.12156990890090058  | Test loss :  0.11830222100279869\n",
      "Epoch:  153 / 200  | Train loss:  0.12156990890087839  | Test loss :  0.11830222100221746\n",
      "Epoch:  154 / 200  | Train loss:  0.12156990890085885  | Test loss :  0.11830222100171352\n",
      "Epoch:  155 / 200  | Train loss:  0.12156990890084156  | Test loss :  0.11830222100127649\n",
      "Epoch:  156 / 200  | Train loss:  0.12156990890082624  | Test loss :  0.11830222100089749\n",
      "Epoch:  157 / 200  | Train loss:  0.12156990890081261  | Test loss :  0.11830222100056872\n",
      "Epoch:  158 / 200  | Train loss:  0.12156990890080051  | Test loss :  0.11830222100028355\n",
      "Epoch:  159 / 200  | Train loss:  0.12156990890078966  | Test loss :  0.11830222100003603\n",
      "Epoch:  160 / 200  | Train loss:  0.12156990890077991  | Test loss :  0.1183022209998212\n",
      "Epoch:  161 / 200  | Train loss:  0.12156990890077111  | Test loss :  0.11830222099963468\n",
      "Epoch:  162 / 200  | Train loss:  0.12156990890076318  | Test loss :  0.11830222099947271\n",
      "Epoch:  163 / 200  | Train loss:  0.12156990890075596  | Test loss :  0.11830222099933196\n",
      "Epoch:  164 / 200  | Train loss:  0.12156990890074937  | Test loss :  0.11830222099920967\n",
      "Epoch:  165 / 200  | Train loss:  0.12156990890074328  | Test loss :  0.1183022209991033\n",
      "Epoch:  166 / 200  | Train loss:  0.1215699089007377  | Test loss :  0.11830222099901072\n",
      "Epoch:  167 / 200  | Train loss:  0.1215699089007325  | Test loss :  0.11830222099893013\n",
      "Epoch:  168 / 200  | Train loss:  0.12156990890072768  | Test loss :  0.11830222099885994\n",
      "Epoch:  169 / 200  | Train loss:  0.12156990890072313  | Test loss :  0.11830222099879871\n",
      "Epoch:  170 / 200  | Train loss:  0.12156990890071884  | Test loss :  0.11830222099874527\n",
      "Epoch:  171 / 200  | Train loss:  0.12156990890071484  | Test loss :  0.11830222099869857\n",
      "Epoch:  172 / 200  | Train loss:  0.121569908900711  | Test loss :  0.11830222099865771\n",
      "Epoch:  173 / 200  | Train loss:  0.12156990890070735  | Test loss :  0.11830222099862191\n",
      "Epoch:  174 / 200  | Train loss:  0.1215699089007038  | Test loss :  0.11830222099859052\n",
      "Epoch:  175 / 200  | Train loss:  0.12156990890070044  | Test loss :  0.1183022209985629\n",
      "Epoch:  176 / 200  | Train loss:  0.12156990890069713  | Test loss :  0.1183022209985386\n",
      "Epoch:  177 / 200  | Train loss:  0.12156990890069395  | Test loss :  0.11830222099851713\n",
      "Epoch:  178 / 200  | Train loss:  0.12156990890069087  | Test loss :  0.11830222099849814\n",
      "Epoch:  179 / 200  | Train loss:  0.12156990890068785  | Test loss :  0.11830222099848128\n",
      "Epoch:  180 / 200  | Train loss:  0.1215699089006849  | Test loss :  0.1183022209984663\n",
      "Epoch:  181 / 200  | Train loss:  0.12156990890068196  | Test loss :  0.1183022209984529\n",
      "Epoch:  182 / 200  | Train loss:  0.1215699089006791  | Test loss :  0.11830222099844091\n",
      "Epoch:  183 / 200  | Train loss:  0.12156990890067634  | Test loss :  0.11830222099843012\n",
      "Epoch:  184 / 200  | Train loss:  0.12156990890067354  | Test loss :  0.11830222099842036\n",
      "Epoch:  185 / 200  | Train loss:  0.12156990890067076  | Test loss :  0.11830222099841152\n",
      "Epoch:  186 / 200  | Train loss:  0.12156990890066807  | Test loss :  0.11830222099840348\n",
      "Epoch:  187 / 200  | Train loss:  0.12156990890066532  | Test loss :  0.1183022209983961\n",
      "Epoch:  188 / 200  | Train loss:  0.12156990890066267  | Test loss :  0.11830222099838932\n",
      "Epoch:  189 / 200  | Train loss:  0.12156990890066001  | Test loss :  0.118302220998383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  190 / 200  | Train loss:  0.12156990890065733  | Test loss :  0.1183022209983772\n",
      "Epoch:  191 / 200  | Train loss:  0.1215699089006547  | Test loss :  0.11830222099837176\n",
      "Epoch:  192 / 200  | Train loss:  0.12156990890065208  | Test loss :  0.11830222099836661\n",
      "Epoch:  193 / 200  | Train loss:  0.1215699089006495  | Test loss :  0.11830222099836178\n",
      "Epoch:  194 / 200  | Train loss:  0.1215699089006469  | Test loss :  0.11830222099835719\n",
      "Epoch:  195 / 200  | Train loss:  0.1215699089006443  | Test loss :  0.11830222099835282\n",
      "Epoch:  196 / 200  | Train loss:  0.12156990890064165  | Test loss :  0.11830222099834863\n",
      "Epoch:  197 / 200  | Train loss:  0.12156990890063911  | Test loss :  0.11830222099834461\n",
      "Epoch:  198 / 200  | Train loss:  0.12156990890063651  | Test loss :  0.11830222099834073\n",
      "Epoch:  199 / 200  | Train loss:  0.12156990890063392  | Test loss :  0.11830222099833698\n",
      "Epoch:  200 / 200  | Train loss:  0.12156990890063134  | Test loss :  0.1183022209983333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1183022209983333"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_path = './MNIST_Sub/train_data.txt'\n",
    "train_labels_path = './MNIST_Sub/train_labels.txt'\n",
    "test_data_path = './MNIST_Sub/test_data.txt'\n",
    "test_labels_path = './MNIST_Sub/test_labels.txt'\n",
    "\n",
    "# regressor\n",
    "regressor_layers_arch = [['Linear', (1, 16)], ['ReLU'], ['Linear', (16, 16)], ['ReLU'], ['Linear', (16, 1)]]\n",
    "def data_function(x):\n",
    "    return np.power(x,3) + pow(x,2) + 1\n",
    "regressor = Regressor(regressor_layers_arch, data_function, learning_rate = 1e-4, batch_size = 32, max_epoch = 200)\n",
    "regressor.Train()\n",
    "\n",
    "regressor.Test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Classfication Network\n",
    "Implement your own classifier with gradient descent. Please read the requirement for Task 5 in the pdf document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def One_Hot_Encode(labels, classes = 10):\n",
    "    '''\n",
    "    /*  Make the labels one-hot.\n",
    "     *  For example, if there are 5 classes {0, 1, 2, 3, 4} then\n",
    "     *  [0, 2, 4] -> [[1, 0, 0, 0, 0],\n",
    "     * \t\t\t\t\t\t\t\t[0, 0, 1, 0, 0],\n",
    "     * \t\t\t\t\t\t\t\t[0, 0, 0, 0, 1]]\n",
    "     */\n",
    "    '''\n",
    "    \n",
    "    ########## Code start  ##########\n",
    "    # Creates the classifier matrix that is number of labels by number of classes\n",
    "    class_matrix = np.zeros((len(labels), classes))\n",
    "    \n",
    "    # Loops through each label and makes the current index in the classifier matrix\n",
    "    # a 1 to properly classify each label\n",
    "    for i in range(len(labels)):\n",
    "        class_matrix[i][labels[i]] = 1\n",
    "    \n",
    "    return class_matrix\n",
    "    ##########  Code end   ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    #Classifier\n",
    "    def __init__(self, train_data_path, train_labels_path, test_data_path, test_labels_path, layers_arch, learning_rate = 1e-3, batch_size = 32, max_epoch = 200, classes = 10):\n",
    "        self.classes = classes\n",
    "\n",
    "        self.train_data_path = train_data_path\n",
    "        self.train_labels_path = train_labels_path\n",
    "        self.test_data_path = test_data_path\n",
    "        self.test_labels_path = test_labels_path\n",
    "\n",
    "\n",
    "        self.train_data = [] #The shape of train data should be (n_samples,28^2)\n",
    "        self.train_labels = []\n",
    "        self.test_data = []\n",
    "        self.test_labels = []\n",
    "        \n",
    "        self.layers_arch = layers_arch\n",
    "        self.net = Network(layers_arch)\n",
    "        self.loss_function = MSELoss()\n",
    "\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "\n",
    "    def dataloader(self):\n",
    "\n",
    "        with open(self.train_data_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                self.train_data.append(np.array(line.strip().split()).astype(np.float64)/255.0)\n",
    "        self.train_data = np.array(self.train_data)\n",
    "\n",
    "        with open(self.train_labels_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                self.train_labels.append(int(line.strip()))\n",
    "        self.train_labels = np.array(self.train_labels)\n",
    "\n",
    "        with open(self.test_data_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                self.test_data.append(np.array(line.strip().split()).astype(np.float64)/255.0)\n",
    "        self.test_data = np.array(self.test_data)\n",
    "\n",
    "        with open(self.test_labels_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                self.test_labels.append(int(line.strip()))\n",
    "        self.test_labels = np.array(self.test_labels)\n",
    "\n",
    "\n",
    "    def Train_One_Epoch(self):\n",
    "        '''\n",
    "        Here we train the network using gradient descent\n",
    "        '''\n",
    "        loss = 0\n",
    "        n_loop = int(math.ceil(len(self.train_data) / self.batch_size))\n",
    "        for i in range(n_loop):\n",
    "            batch_data = self.train_data[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "            batch_label = self.train_labels[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "            batch_one_hot_label = One_Hot_Encode(batch_label, classes = self.classes)\n",
    "            \n",
    "            '''\n",
    "             /*  Forward the data to the network.\n",
    "             *  Forward the result to the loss function.\n",
    "             *  Backward.\n",
    "             *  Update the weights with weight gradients.\n",
    "             *  Do not forget the learning rate!\n",
    "             */\n",
    "            '''\n",
    "            \n",
    "            ########## Code start  ##########\n",
    "            # Based off of the sample code provided in Checkpoint 3\n",
    "            # Calculates the predicted value from the inputted batch of data with the forward method\n",
    "            prediction = self.net.forward(batch_data)\n",
    "            # Calculates and adds the loss from the prediction and true values to the loss accumulator\n",
    "            loss += self.loss_function.forward(prediction, batch_one_hot_label)\n",
    "\n",
    "            # Calculates the predicted gradient from the inputted batch of data with the backward method\n",
    "            pred_grad = self.loss_function.backward()\n",
    "            # Performs the backward method on the network with the predicted gradient\n",
    "            self.net.backward(pred_grad)\n",
    "            \n",
    "            # Loops through each layer in the network's architecture and calculates the parameter\n",
    "            # weights for each linear layer\n",
    "            for i in range(len(self.layers_arch)):\n",
    "                if self.layers_arch[i][0] == 'Linear':\n",
    "                    self.net.layers[i].W -= self.net.layers[i].W_grad * self.learning_rate\n",
    "            ##########  Code end   ##########\n",
    "        \n",
    "        return loss / n_loop\n",
    "\n",
    "    def Test(self):\n",
    "        '''\n",
    "        the class with max score is our predicted label\n",
    "        '''\n",
    "        score = self.net.forward(self.test_data)\n",
    "        accuracy = 0\n",
    "        for i in range(np.shape(score)[0]):\n",
    "            one_label_list = score[i].tolist()\n",
    "            label_pred = one_label_list.index(max(one_label_list))\n",
    "            if label_pred == self.test_labels[i]:\n",
    "                accuracy = accuracy +1\n",
    "\n",
    "        accuracy = accuracy/np.shape(score)[0]\n",
    "        return accuracy\n",
    "\n",
    "    def Train(self):\n",
    "        self.dataloader()\n",
    "        for i in range(self.max_epoch):\n",
    "            loss = self.Train_One_Epoch()\n",
    "            accuray = self.Test()\n",
    "            print(\"Epoch: \", str(i+1), \"/\", str(self.max_epoch), \" | Train loss: \", loss, \" | Test Accuracy : \", accuray)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "That's it! Congratulations on finishing everything. Now try your network on MNIST!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 / 200  | Train loss:  1.2142269194468032  | Test Accuracy :  0.46\n",
      "Epoch:  2 / 200  | Train loss:  0.7613363660672452  | Test Accuracy :  0.57\n",
      "Epoch:  3 / 200  | Train loss:  0.6293032932778765  | Test Accuracy :  0.665\n",
      "Epoch:  4 / 200  | Train loss:  0.5549495099811881  | Test Accuracy :  0.715\n",
      "Epoch:  5 / 200  | Train loss:  0.504091596786214  | Test Accuracy :  0.715\n",
      "Epoch:  6 / 200  | Train loss:  0.4659019889015074  | Test Accuracy :  0.715\n",
      "Epoch:  7 / 200  | Train loss:  0.43562967110705497  | Test Accuracy :  0.74\n",
      "Epoch:  8 / 200  | Train loss:  0.41066264912035266  | Test Accuracy :  0.75\n",
      "Epoch:  9 / 200  | Train loss:  0.3893637875099155  | Test Accuracy :  0.75\n",
      "Epoch:  10 / 200  | Train loss:  0.3709858754287907  | Test Accuracy :  0.75\n",
      "Epoch:  11 / 200  | Train loss:  0.35492586212334987  | Test Accuracy :  0.75\n",
      "Epoch:  12 / 200  | Train loss:  0.34055643223733123  | Test Accuracy :  0.76\n",
      "Epoch:  13 / 200  | Train loss:  0.327625544885862  | Test Accuracy :  0.76\n",
      "Epoch:  14 / 200  | Train loss:  0.3158057275111289  | Test Accuracy :  0.765\n",
      "Epoch:  15 / 200  | Train loss:  0.305019201683898  | Test Accuracy :  0.765\n",
      "Epoch:  16 / 200  | Train loss:  0.2950781198104894  | Test Accuracy :  0.765\n",
      "Epoch:  17 / 200  | Train loss:  0.2858692721202075  | Test Accuracy :  0.78\n",
      "Epoch:  18 / 200  | Train loss:  0.2773036788006762  | Test Accuracy :  0.785\n",
      "Epoch:  19 / 200  | Train loss:  0.26925852788031435  | Test Accuracy :  0.795\n",
      "Epoch:  20 / 200  | Train loss:  0.26168357462005204  | Test Accuracy :  0.795\n",
      "Epoch:  21 / 200  | Train loss:  0.25460349042611885  | Test Accuracy :  0.79\n",
      "Epoch:  22 / 200  | Train loss:  0.24787674006576524  | Test Accuracy :  0.79\n",
      "Epoch:  23 / 200  | Train loss:  0.2415287224806966  | Test Accuracy :  0.785\n",
      "Epoch:  24 / 200  | Train loss:  0.23548607931576263  | Test Accuracy :  0.785\n",
      "Epoch:  25 / 200  | Train loss:  0.2297566948514594  | Test Accuracy :  0.79\n",
      "Epoch:  26 / 200  | Train loss:  0.22432806019599158  | Test Accuracy :  0.79\n",
      "Epoch:  27 / 200  | Train loss:  0.21912341435025012  | Test Accuracy :  0.8\n",
      "Epoch:  28 / 200  | Train loss:  0.214198723545638  | Test Accuracy :  0.805\n",
      "Epoch:  29 / 200  | Train loss:  0.20951886336393152  | Test Accuracy :  0.815\n",
      "Epoch:  30 / 200  | Train loss:  0.20502014517853112  | Test Accuracy :  0.815\n",
      "Epoch:  31 / 200  | Train loss:  0.20073683311415497  | Test Accuracy :  0.815\n",
      "Epoch:  32 / 200  | Train loss:  0.19661378492582118  | Test Accuracy :  0.815\n",
      "Epoch:  33 / 200  | Train loss:  0.1926562887549405  | Test Accuracy :  0.815\n",
      "Epoch:  34 / 200  | Train loss:  0.18886344412720327  | Test Accuracy :  0.815\n",
      "Epoch:  35 / 200  | Train loss:  0.1851637925895586  | Test Accuracy :  0.82\n",
      "Epoch:  36 / 200  | Train loss:  0.18163958524898854  | Test Accuracy :  0.82\n",
      "Epoch:  37 / 200  | Train loss:  0.17822676226374498  | Test Accuracy :  0.82\n",
      "Epoch:  38 / 200  | Train loss:  0.17494603292098743  | Test Accuracy :  0.815\n",
      "Epoch:  39 / 200  | Train loss:  0.17178281952276564  | Test Accuracy :  0.82\n",
      "Epoch:  40 / 200  | Train loss:  0.16873947186014565  | Test Accuracy :  0.82\n",
      "Epoch:  41 / 200  | Train loss:  0.1657697734276931  | Test Accuracy :  0.82\n",
      "Epoch:  42 / 200  | Train loss:  0.1629112514868104  | Test Accuracy :  0.83\n",
      "Epoch:  43 / 200  | Train loss:  0.16011260772433722  | Test Accuracy :  0.835\n",
      "Epoch:  44 / 200  | Train loss:  0.15742783705215163  | Test Accuracy :  0.835\n",
      "Epoch:  45 / 200  | Train loss:  0.15481846166063376  | Test Accuracy :  0.835\n",
      "Epoch:  46 / 200  | Train loss:  0.15230886648668523  | Test Accuracy :  0.835\n",
      "Epoch:  47 / 200  | Train loss:  0.1498430144850816  | Test Accuracy :  0.84\n",
      "Epoch:  48 / 200  | Train loss:  0.14744070100015552  | Test Accuracy :  0.84\n",
      "Epoch:  49 / 200  | Train loss:  0.14514082636616374  | Test Accuracy :  0.845\n",
      "Epoch:  50 / 200  | Train loss:  0.14284559819275686  | Test Accuracy :  0.84\n",
      "Epoch:  51 / 200  | Train loss:  0.14066248565712602  | Test Accuracy :  0.845\n",
      "Epoch:  52 / 200  | Train loss:  0.1385295170741034  | Test Accuracy :  0.845\n",
      "Epoch:  53 / 200  | Train loss:  0.13644753439917334  | Test Accuracy :  0.845\n",
      "Epoch:  54 / 200  | Train loss:  0.13440820291363906  | Test Accuracy :  0.85\n",
      "Epoch:  55 / 200  | Train loss:  0.1324455551404053  | Test Accuracy :  0.85\n",
      "Epoch:  56 / 200  | Train loss:  0.13049166626609932  | Test Accuracy :  0.85\n",
      "Epoch:  57 / 200  | Train loss:  0.12860544745775404  | Test Accuracy :  0.85\n",
      "Epoch:  58 / 200  | Train loss:  0.12676558485365905  | Test Accuracy :  0.855\n",
      "Epoch:  59 / 200  | Train loss:  0.12498547283284664  | Test Accuracy :  0.855\n",
      "Epoch:  60 / 200  | Train loss:  0.12321317068327717  | Test Accuracy :  0.855\n",
      "Epoch:  61 / 200  | Train loss:  0.12150905579619722  | Test Accuracy :  0.855\n",
      "Epoch:  62 / 200  | Train loss:  0.11983179290076788  | Test Accuracy :  0.855\n",
      "Epoch:  63 / 200  | Train loss:  0.11819375278587989  | Test Accuracy :  0.855\n",
      "Epoch:  64 / 200  | Train loss:  0.11660504637064313  | Test Accuracy :  0.855\n",
      "Epoch:  65 / 200  | Train loss:  0.11503114049623965  | Test Accuracy :  0.855\n",
      "Epoch:  66 / 200  | Train loss:  0.1135006088066972  | Test Accuracy :  0.86\n",
      "Epoch:  67 / 200  | Train loss:  0.1120077100244021  | Test Accuracy :  0.86\n",
      "Epoch:  68 / 200  | Train loss:  0.11053851684304285  | Test Accuracy :  0.86\n",
      "Epoch:  69 / 200  | Train loss:  0.10910154432860666  | Test Accuracy :  0.86\n",
      "Epoch:  70 / 200  | Train loss:  0.10770173252658559  | Test Accuracy :  0.865\n",
      "Epoch:  71 / 200  | Train loss:  0.10631378721553347  | Test Accuracy :  0.865\n",
      "Epoch:  72 / 200  | Train loss:  0.10498476434612695  | Test Accuracy :  0.865\n",
      "Epoch:  73 / 200  | Train loss:  0.10366329283370075  | Test Accuracy :  0.865\n",
      "Epoch:  74 / 200  | Train loss:  0.10237112110490003  | Test Accuracy :  0.865\n",
      "Epoch:  75 / 200  | Train loss:  0.10109972934925966  | Test Accuracy :  0.865\n",
      "Epoch:  76 / 200  | Train loss:  0.09985023979918693  | Test Accuracy :  0.865\n",
      "Epoch:  77 / 200  | Train loss:  0.09863412504689568  | Test Accuracy :  0.865\n",
      "Epoch:  78 / 200  | Train loss:  0.09743471526648799  | Test Accuracy :  0.865\n",
      "Epoch:  79 / 200  | Train loss:  0.09627602249550311  | Test Accuracy :  0.865\n",
      "Epoch:  80 / 200  | Train loss:  0.0951166604138425  | Test Accuracy :  0.865\n",
      "Epoch:  81 / 200  | Train loss:  0.0939797864959033  | Test Accuracy :  0.865\n",
      "Epoch:  82 / 200  | Train loss:  0.09289526656482905  | Test Accuracy :  0.865\n",
      "Epoch:  83 / 200  | Train loss:  0.09180307929688755  | Test Accuracy :  0.865\n",
      "Epoch:  84 / 200  | Train loss:  0.09072430873833942  | Test Accuracy :  0.865\n",
      "Epoch:  85 / 200  | Train loss:  0.0896764322509014  | Test Accuracy :  0.865\n",
      "Epoch:  86 / 200  | Train loss:  0.08864817140252254  | Test Accuracy :  0.865\n",
      "Epoch:  87 / 200  | Train loss:  0.08765195710223599  | Test Accuracy :  0.865\n",
      "Epoch:  88 / 200  | Train loss:  0.08666352975997825  | Test Accuracy :  0.865\n",
      "Epoch:  89 / 200  | Train loss:  0.08567315871886468  | Test Accuracy :  0.865\n",
      "Epoch:  90 / 200  | Train loss:  0.08470926880120891  | Test Accuracy :  0.865\n",
      "Epoch:  91 / 200  | Train loss:  0.0837841780005521  | Test Accuracy :  0.87\n",
      "Epoch:  92 / 200  | Train loss:  0.08284405172266043  | Test Accuracy :  0.87\n",
      "Epoch:  93 / 200  | Train loss:  0.08194539761582344  | Test Accuracy :  0.87\n",
      "Epoch:  94 / 200  | Train loss:  0.08104022039431721  | Test Accuracy :  0.87\n",
      "Epoch:  95 / 200  | Train loss:  0.08017157625322117  | Test Accuracy :  0.87\n",
      "Epoch:  96 / 200  | Train loss:  0.07930370950849458  | Test Accuracy :  0.87\n",
      "Epoch:  97 / 200  | Train loss:  0.0784442558833362  | Test Accuracy :  0.87\n",
      "Epoch:  98 / 200  | Train loss:  0.0776094134998264  | Test Accuracy :  0.87\n",
      "Epoch:  99 / 200  | Train loss:  0.07678221356055917  | Test Accuracy :  0.87\n",
      "Epoch:  100 / 200  | Train loss:  0.07597596762644233  | Test Accuracy :  0.87\n",
      "Epoch:  101 / 200  | Train loss:  0.07517411858111334  | Test Accuracy :  0.865\n",
      "Epoch:  102 / 200  | Train loss:  0.07438072867694877  | Test Accuracy :  0.865\n",
      "Epoch:  103 / 200  | Train loss:  0.07360301884720233  | Test Accuracy :  0.865\n",
      "Epoch:  104 / 200  | Train loss:  0.07284771973310007  | Test Accuracy :  0.865\n",
      "Epoch:  105 / 200  | Train loss:  0.07208706509143582  | Test Accuracy :  0.865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  106 / 200  | Train loss:  0.0713612091737407  | Test Accuracy :  0.865\n",
      "Epoch:  107 / 200  | Train loss:  0.07061873238746992  | Test Accuracy :  0.865\n",
      "Epoch:  108 / 200  | Train loss:  0.0699006816315759  | Test Accuracy :  0.865\n",
      "Epoch:  109 / 200  | Train loss:  0.06920041746107887  | Test Accuracy :  0.865\n",
      "Epoch:  110 / 200  | Train loss:  0.0685021539918214  | Test Accuracy :  0.865\n",
      "Epoch:  111 / 200  | Train loss:  0.06781387854288813  | Test Accuracy :  0.865\n",
      "Epoch:  112 / 200  | Train loss:  0.06714989616110159  | Test Accuracy :  0.865\n",
      "Epoch:  113 / 200  | Train loss:  0.06648130319824001  | Test Accuracy :  0.865\n",
      "Epoch:  114 / 200  | Train loss:  0.06581996920012713  | Test Accuracy :  0.865\n",
      "Epoch:  115 / 200  | Train loss:  0.06517106664550901  | Test Accuracy :  0.865\n",
      "Epoch:  116 / 200  | Train loss:  0.06453693892886735  | Test Accuracy :  0.865\n",
      "Epoch:  117 / 200  | Train loss:  0.06390981505677566  | Test Accuracy :  0.86\n",
      "Epoch:  118 / 200  | Train loss:  0.06328781352450642  | Test Accuracy :  0.865\n",
      "Epoch:  119 / 200  | Train loss:  0.06268081723350646  | Test Accuracy :  0.86\n",
      "Epoch:  120 / 200  | Train loss:  0.06207535085840882  | Test Accuracy :  0.865\n",
      "Epoch:  121 / 200  | Train loss:  0.06148359713777846  | Test Accuracy :  0.865\n",
      "Epoch:  122 / 200  | Train loss:  0.06090133726695633  | Test Accuracy :  0.86\n",
      "Epoch:  123 / 200  | Train loss:  0.060325065840871724  | Test Accuracy :  0.86\n",
      "Epoch:  124 / 200  | Train loss:  0.05975196471644536  | Test Accuracy :  0.86\n",
      "Epoch:  125 / 200  | Train loss:  0.05919178857863883  | Test Accuracy :  0.86\n",
      "Epoch:  126 / 200  | Train loss:  0.05862595110585448  | Test Accuracy :  0.86\n",
      "Epoch:  127 / 200  | Train loss:  0.05809830059917192  | Test Accuracy :  0.86\n",
      "Epoch:  128 / 200  | Train loss:  0.0575478688546953  | Test Accuracy :  0.86\n",
      "Epoch:  129 / 200  | Train loss:  0.057013562834580575  | Test Accuracy :  0.86\n",
      "Epoch:  130 / 200  | Train loss:  0.05649893206412784  | Test Accuracy :  0.86\n",
      "Epoch:  131 / 200  | Train loss:  0.0559666713679438  | Test Accuracy :  0.86\n",
      "Epoch:  132 / 200  | Train loss:  0.055458531723428114  | Test Accuracy :  0.86\n",
      "Epoch:  133 / 200  | Train loss:  0.05495318323141766  | Test Accuracy :  0.86\n",
      "Epoch:  134 / 200  | Train loss:  0.054464271149018605  | Test Accuracy :  0.86\n",
      "Epoch:  135 / 200  | Train loss:  0.05396137717754687  | Test Accuracy :  0.86\n",
      "Epoch:  136 / 200  | Train loss:  0.05348116092455508  | Test Accuracy :  0.86\n",
      "Epoch:  137 / 200  | Train loss:  0.05300472620038506  | Test Accuracy :  0.86\n",
      "Epoch:  138 / 200  | Train loss:  0.05252912155724692  | Test Accuracy :  0.86\n",
      "Epoch:  139 / 200  | Train loss:  0.052059768015374965  | Test Accuracy :  0.86\n",
      "Epoch:  140 / 200  | Train loss:  0.05160216553490722  | Test Accuracy :  0.86\n",
      "Epoch:  141 / 200  | Train loss:  0.05113972104139622  | Test Accuracy :  0.86\n",
      "Epoch:  142 / 200  | Train loss:  0.05069416061406318  | Test Accuracy :  0.86\n",
      "Epoch:  143 / 200  | Train loss:  0.050239508840632746  | Test Accuracy :  0.86\n",
      "Epoch:  144 / 200  | Train loss:  0.049800236955068  | Test Accuracy :  0.86\n",
      "Epoch:  145 / 200  | Train loss:  0.04936922582797388  | Test Accuracy :  0.86\n",
      "Epoch:  146 / 200  | Train loss:  0.04893641204068145  | Test Accuracy :  0.86\n",
      "Epoch:  147 / 200  | Train loss:  0.0485128844954845  | Test Accuracy :  0.86\n",
      "Epoch:  148 / 200  | Train loss:  0.04809190698127235  | Test Accuracy :  0.86\n",
      "Epoch:  149 / 200  | Train loss:  0.04767021166851903  | Test Accuracy :  0.86\n",
      "Epoch:  150 / 200  | Train loss:  0.04726533653951203  | Test Accuracy :  0.86\n",
      "Epoch:  151 / 200  | Train loss:  0.04685636659304819  | Test Accuracy :  0.86\n",
      "Epoch:  152 / 200  | Train loss:  0.04646801936711422  | Test Accuracy :  0.86\n",
      "Epoch:  153 / 200  | Train loss:  0.04605883735970598  | Test Accuracy :  0.86\n",
      "Epoch:  154 / 200  | Train loss:  0.04567542833575808  | Test Accuracy :  0.86\n",
      "Epoch:  155 / 200  | Train loss:  0.04528805309799771  | Test Accuracy :  0.86\n",
      "Epoch:  156 / 200  | Train loss:  0.04490842008188258  | Test Accuracy :  0.86\n",
      "Epoch:  157 / 200  | Train loss:  0.04454074174862235  | Test Accuracy :  0.86\n",
      "Epoch:  158 / 200  | Train loss:  0.044159903467613414  | Test Accuracy :  0.86\n",
      "Epoch:  159 / 200  | Train loss:  0.04379125659148499  | Test Accuracy :  0.86\n",
      "Epoch:  160 / 200  | Train loss:  0.043425731212168556  | Test Accuracy :  0.86\n",
      "Epoch:  161 / 200  | Train loss:  0.043065122923027464  | Test Accuracy :  0.86\n",
      "Epoch:  162 / 200  | Train loss:  0.04271220856441752  | Test Accuracy :  0.86\n",
      "Epoch:  163 / 200  | Train loss:  0.04235383639467323  | Test Accuracy :  0.86\n",
      "Epoch:  164 / 200  | Train loss:  0.042008138522719375  | Test Accuracy :  0.86\n",
      "Epoch:  165 / 200  | Train loss:  0.0416603299532076  | Test Accuracy :  0.86\n",
      "Epoch:  166 / 200  | Train loss:  0.04132250469596626  | Test Accuracy :  0.86\n",
      "Epoch:  167 / 200  | Train loss:  0.040985827038849135  | Test Accuracy :  0.86\n",
      "Epoch:  168 / 200  | Train loss:  0.04065585231053957  | Test Accuracy :  0.86\n",
      "Epoch:  169 / 200  | Train loss:  0.04032061133674674  | Test Accuracy :  0.86\n",
      "Epoch:  170 / 200  | Train loss:  0.03999895003850195  | Test Accuracy :  0.86\n",
      "Epoch:  171 / 200  | Train loss:  0.03967985124691319  | Test Accuracy :  0.86\n",
      "Epoch:  172 / 200  | Train loss:  0.03936279069968616  | Test Accuracy :  0.86\n",
      "Epoch:  173 / 200  | Train loss:  0.03905352820136923  | Test Accuracy :  0.86\n",
      "Epoch:  174 / 200  | Train loss:  0.03873623320599499  | Test Accuracy :  0.86\n",
      "Epoch:  175 / 200  | Train loss:  0.038431871457804724  | Test Accuracy :  0.86\n",
      "Epoch:  176 / 200  | Train loss:  0.03812589796349661  | Test Accuracy :  0.86\n",
      "Epoch:  177 / 200  | Train loss:  0.037829993320072  | Test Accuracy :  0.86\n",
      "Epoch:  178 / 200  | Train loss:  0.03753217283923455  | Test Accuracy :  0.86\n",
      "Epoch:  179 / 200  | Train loss:  0.037238335238223744  | Test Accuracy :  0.86\n",
      "Epoch:  180 / 200  | Train loss:  0.03694179506189708  | Test Accuracy :  0.86\n",
      "Epoch:  181 / 200  | Train loss:  0.03665872450104733  | Test Accuracy :  0.86\n",
      "Epoch:  182 / 200  | Train loss:  0.03637447725246451  | Test Accuracy :  0.86\n",
      "Epoch:  183 / 200  | Train loss:  0.036087752550107204  | Test Accuracy :  0.86\n",
      "Epoch:  184 / 200  | Train loss:  0.03580449665633111  | Test Accuracy :  0.86\n",
      "Epoch:  185 / 200  | Train loss:  0.03553475233086072  | Test Accuracy :  0.86\n",
      "Epoch:  186 / 200  | Train loss:  0.03525682586288075  | Test Accuracy :  0.86\n",
      "Epoch:  187 / 200  | Train loss:  0.034988766575770415  | Test Accuracy :  0.86\n",
      "Epoch:  188 / 200  | Train loss:  0.03471531709994799  | Test Accuracy :  0.86\n",
      "Epoch:  189 / 200  | Train loss:  0.03445616940721907  | Test Accuracy :  0.86\n",
      "Epoch:  190 / 200  | Train loss:  0.034187469591919574  | Test Accuracy :  0.86\n",
      "Epoch:  191 / 200  | Train loss:  0.03393014306874189  | Test Accuracy :  0.86\n",
      "Epoch:  192 / 200  | Train loss:  0.03367227330458468  | Test Accuracy :  0.86\n",
      "Epoch:  193 / 200  | Train loss:  0.03341422973614084  | Test Accuracy :  0.86\n",
      "Epoch:  194 / 200  | Train loss:  0.03316120104727096  | Test Accuracy :  0.86\n",
      "Epoch:  195 / 200  | Train loss:  0.032910762505542826  | Test Accuracy :  0.86\n",
      "Epoch:  196 / 200  | Train loss:  0.03266545350659578  | Test Accuracy :  0.86\n",
      "Epoch:  197 / 200  | Train loss:  0.03242078231637058  | Test Accuracy :  0.86\n",
      "Epoch:  198 / 200  | Train loss:  0.03217597429677223  | Test Accuracy :  0.86\n",
      "Epoch:  199 / 200  | Train loss:  0.03193581244735701  | Test Accuracy :  0.86\n",
      "Epoch:  200 / 200  | Train loss:  0.03169677814263984  | Test Accuracy :  0.86\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.86"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_path = './MNIST_Sub/train_data.txt'\n",
    "train_labels_path = './MNIST_Sub/train_labels.txt'\n",
    "test_data_path = './MNIST_Sub/test_data.txt'\n",
    "test_labels_path = './MNIST_Sub/test_labels.txt'\n",
    "\n",
    "\n",
    "#classifier\n",
    "classifier_layers_arch = [['Linear', (28*28, 256)], ['ReLU'], ['Linear', (256, 10)]]\n",
    "cls = Classifier(train_data_path, train_labels_path, test_data_path, test_labels_path, layers_arch = classifier_layers_arch, learning_rate = 0.01, batch_size = 32, max_epoch = 200)\n",
    "cls.Train()\n",
    "cls.Test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
